[
{
	"uri": "https://lczen.github.io/en/showcase/hugo/hugo-theme-zzo/",
	"title": "Hugo Zzo Theme",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Make a blog with hugo zzo theme!",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/showcase/hugo/hugo-theme-zdoc/",
	"title": "Hugo zDoc Theme",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Make a documentation with hugo zdoc theme!",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/",
	"title": "",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/categories/",
	"title": "Categories",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/",
	"title": "Posts",
	"tags": ["index"],
	"categories": [],
	"series": [],
	"description": "Post page",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/tags/",
	"title": "Tags",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/",
	"title": "机器学习",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/",
	"title": "机器学习",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/ml/",
	"title": "机器学习",
	"tags": ["机器学习"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "对数损失/二元交叉熵损失 熵 "
},
{
	"uri": "https://lczen.github.io/en/posts/pandas_view_pratice4/",
	"title": "Pandas_view_pratice4",
	"tags": [""],
	"categories": [],
	"series": [""],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/series/",
	"title": "Series",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/tags/hadoop/",
	"title": "hadoop",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/hadoop/",
	"title": "hadoop",
	"tags": ["hadoop"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "HDFS简介与常见命令 HDFS 全称Hadoop distributed file system，简称HDFS，是一个分布式文件系统。它是谷歌的GFS提出之后出现的另外一种文件系统。它有一定高度的容错性，而且提供了高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS 提供了一个高度容错性和高吞吐量的海量数据存储解决方案。\n在最初，HADOOP是作为Apache Nutch搜索引擎项目的基础架构而开发的，后来由于它独有的特性，让它成为HADOOP CORE项目的一部分。\nHDFS的设计思路 简单说一下它的设计思路，大家就可以理解为什么它可以提供高吞吐量的数据访问和适合大规模数据集的应用的特性。\n首先HDFS的设计之初就是针对超大文件的存储的，小文件不会提高访问和存储速度，反而会降低；其次它采用了最高效的访问模式，也就是经常所说的流式数据访问，特点就是一次写入多次读取；再有就是它运行在普通的硬件之上的，即使硬件故障，也就通过容错来保证数据的高可用。\nHDFS核心概念  Block：大文件的存储会被分割为多个block进行存储。默认为64MB，每一个blok会在多个datanode上存储多份副本，默认为3份。这些设置都能够通过配置文件进行更改。 Namenode：主要负责存储一些metadata信息，主要包括文件目录、block和文件对应关系，以及block和datanode的对应关系 Datanode：负责存储数据，上面我们所说的高度的容错性大部分在datanode上实现的，还有一部分容错性是体现在namenode和secondname，还有jobtracker的容错等。  HDFS的基础架构图 HDFS优点：  高吞吐量访问：HDFS的每个block分布在不同的rack上，在用户访问时，HDFS会计算使用最近和访问量最小的服务器给用户提供。由于block在不同的rack上都有备份，所以不再是单数据访问，所以速度和效率是非常快的。另外HDFS可以并行从服务器集群中读写，增加了文件读写的访问带宽。 高容错性：上面简单的介绍了一下高度容错。系统故障是不可避免的，如何做到故障之后的数据恢复和容错处理是至关重要的。HDFS通过多方面保证数据的可靠性，多分复制并且分布到物理位置的不同服务器上，数据校验功能、后台的连续自检数据一致性功能，都为高容错提供了可能。 容量扩充：因为HDFS的block信息存放到namenode上，文件的block分布到datanode上，当扩充的时候，仅仅添加datanode数量，系统可以在不停止服务的情况下做扩充，不需要人工干预。\n3个不同的位置： 本地电脑 服务器(Mac:terminal/终端 Windows:Xshell scureCRT) 集群\nHDFS上的数据，分布在不同的地方，我们有一些命令可以用于 增加/查看/删除 等数据操作。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  #显示/下的所有文件夹信息 hadoop fs -ls / #递归显示所有文件夹和子文件(夹) hadoop fs -lsr #创建/user/hadoop目录 hadoop fs -mkdir /user/hadoop #把a.txt放到集群/user/hadoop/文件夹下 hadoop fs -put a.txt /user/hadoop/ #把集群上的/user/hadoop/a.txt拉到本地/目录下 hadoop fs -get /user/hadoop/a.txt / #集群上复制文件 hadoop fs -cp src dst #集群上移动文件 hadoop fs -mv src dst #查看集群上文件/user/hadoop/a.txt的内容 hadoop fs -cat /user/hadoop/a.txt #删除集群上/user/hadoop/a.txt文件 hadoop fs -rm /user/hadoop/a.txt #删除目录和目录下所有文件 hadoop fs -rmr /user/hadoop/a.txt #与hadoop fs -put功能类似 hadoop fs -copyFromLocal localsrc dst #将本地文件上传到hdfs，同时删除本地文件 hadoop fs -moveFromLocal localsrc dst   Hadoop经典案例\u0026ndash;词频统计 我们前面提完了用hadoop完成大数据处理的一些基本知识，这次实验咱们一起来学习一下，如何编写Hadoop的map/reduce任务，完成大数据的处理。\n这是一个非常经典的例子，几乎在任何的hadoop教材上都会看到它，即使如此，它依旧是最经典最有代表性的案例，学习大数据处理，可以从先理解清楚它入手。\n总体流程 咱们来看看对特别大的文件统计，整个过程是如何分拆的。\n大家想想词频统计的过程，如果是单机完成，我们需要做的事情是维护一个计数器字典，对每次出现的词，词频+1.但是当数据量非常大的时候，没办法在内存中维护这么大的一个字典，我们就要换一种思路来完成这个任务了，也就是我们所谓的map-reduce过程。\n大体的过程画成图是下面这个样子：\n大概是分成下面几个环节：\n Map阶段  主要完成key-value对生成，这里是每看到一个单词，就输出(单词，1)的kv对   排序阶段  对刚才的kv对进行排序，这样相同单词就在一块儿了   Reduce阶段  对同一个单词的次数进行汇总，得到(词，频次)对    Map阶段代码 流程大家都看清楚啦，咱们来看看用代码如何实现，你猜怎么着，有了hadoop streaming，咱们可以用python脚本完成map和reduce的过程，然后把整个流程跑起来！\n比如咱们map阶段要做的就是把每一个单词和出现1次的信息输出来！所以我们写一个mapper.py文件，具体内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  #coding: utf-8 #!/usr/bin/env python import sys # 从标准输入过来的数据 for line in sys.stdin: # 把首位的空格去掉 line = line.strip() # 把这一行文本切分成单词(按照空格) words = line.split(\u0026#34;\u0026#34;) # 对见到的单词进行次数标注(出现1次) for word in words: print \u0026#39;%s\\t%s\u0026#39; % (word, 1)   对，就这么简单，你看到了，对于输入进来的每一行，我们做完切分之后，都会输出(单词，1)这样一个kv对，表明这个单词出现过。\n排序阶段 中间会有一个对上述结果进行排序的过程，以保证所有相同的单词都在一起，不过不用担心，这个过程是系统会自动完成的，因此不用我们编写额外的代码。\nReduce阶段 接下来就是对map排序后的结果进行汇总了，这个阶段我们可以用一个reducer.py的python脚本来完成，具体完成的任务，就是：\n对于读入的(单词，1)对\n 如果这个单词还没有结束（排序后所有相同的单词都在一起了），我们就对单词的次数+1 如果遇到新单词了，那重新开始对新单词计数  基于上面的想法，我们可以完成以下的reducer.py脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  #coding: utf-8 #!/usr/bin/env python from operator import itemgetter import sys current_word = None current_count = 0 word = None # 依旧是标准输入过来的数据 for line in sys.stdin: # 去除左右空格 line = line.strip() # 按照tab键对行切分，得到word和次数1 word, count = line.split(\u0026#39;\\t\u0026#39;, 1) # 你得到的1是一个字符串，需要对它进行类型转化 try: count = int(count) except ValueError: #如果不能转成数字，输入有问题，调到下一行 continue # 如果本次读取的单词和上一次一样，对次数加1 if current_word == word: current_count += count else: if current_word: # 输出统计结果 print \u0026#39;%s\\t%s\u0026#39; % (current_word, current_count) current_count = count current_word = word # 不要忘了最后一个词哦，也得输出结果 if current_word == word: print \u0026#39;%s\\t%s\u0026#39; % (current_word, current_count)   怎么样，2个脚本是不是很好懂？\n本地模拟测试代码 一般情况下，我们不会一遍遍用hadoop streaming执行任务，去测试脚本写得对不对，这个过程太麻烦了。\n有没有本地可以测试的办法？有！\n我们可以利用linux管道模拟map-reduce的过程！比如我们可以下面这样测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # very basic test user@ubuntu:~$ echo \u0026#34;foo foo quux labs foo bar quux\u0026#34; | /home/hduser/mapper.py foo 1 foo 1 quux 1 labs 1 foo 1 bar 1 quux 1 user@ubuntu:~$ echo \u0026#34;foo foo quux labs foo bar quux\u0026#34; | /home/hduser/mapper.py | sort -k1,1 | /home/hduser/reducer.py bar 1 foo 3 labs 1 quux 2 # 用一本英文电子书作为输入测试一下！比如可以在http://www.gutenberg.org/etext/20417下载到！ user@ubuntu:~$ cat /tmp/gutenberg/20417-8.txt | /home/hduser/mapper.py The 1 Project 1 Gutenberg 1 EBook 1 of 1 # 后面的sort和reducer过程是一样的，自己试一下！   Hadoop集群运行案例 如果测试通过了，我们就可以在集群上运行我们的案例了，我们先从下面3个链接拉取3本电子书。\nThe Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\nThe Notebooks of Leonardo Da Vinci\nUlysses by James Joyce\n我们把它们下载到一个本地路径下，比如/tmp/gutenberg\nuser@ubuntu:~$ ls -l /tmp/gutenberg/ total 3604 -rw-r--r-- 1 hduser hadoop 674566 Feb 3 10:17 pg20417.txt -rw-r--r-- 1 hduser hadoop 1573112 Feb 3 10:18 pg4300.txt -rw-r--r-- 1 hduser hadoop 1423801 Feb 3 10:18 pg5000.txt user@ubuntu:~$ 拷贝文件到HDFS上 我们前面给大家准备好的HDFS命令小抄在这里派上用场了！执行下面的命令：\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -copyFromLocal /tmp/gutenberg /user/hduser/gutenberg hduser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls Found 1 items drwxr-xr-x - user supergroup 0 2016-05-08 17:40 /user/hduser/gutenberg user@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls /user/hduser/gutenberg Found 3 items -rw-r--r-- 3 hduser supergroup 674566 2016-05-10 11:38 /user/hduser/gutenberg/pg20417.txt -rw-r--r-- 3 hduser supergroup 1573112 2016-05-10 11:38 /user/hduser/gutenberg/pg4300.txt -rw-r--r-- 3 hduser supergroup 1423801 2016-05-10 11:38 /user/hduser/gutenberg/pg5000.txt user@ubuntu:/usr/local/hadoop$ 执行map-reduce任务 下面我们就可以用hadoop streaming执行map-reduce任务了，命令行执行\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \\ -file /home/hduser/mapper.py -mapper /home/hduser/mapper.py \\ -file /home/hduser/reducer.py -reducer /home/hduser/reducer.py \\ -input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output 你甚至可以用-D去指定reducer的个数：\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -D mapred.reduce.tasks=16 ... 运行的结果过程输出的信息大概是下面这个样子：\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -mapper /home/hduser/mapper.py -reducer /home/hduser/reducer.py -input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output additionalConfSpec_:null null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming packageJobJar: [/app/hadoop/tmp/hadoop-unjar54543/] [] /tmp/streamjob54544.jar tmpDir=null [...] INFO mapred.FileInputFormat: Total input paths to process : 7 [...] INFO streaming.StreamJob: getLocalDirs(): [/app/hadoop/tmp/mapred/local] [...] INFO streaming.StreamJob: Running job: job_200803031615_0021 [...] [...] INFO streaming.StreamJob: map 0% reduce 0% [...] INFO streaming.StreamJob: map 43% reduce 0% [...] INFO streaming.StreamJob: map 86% reduce 0% [...] INFO streaming.StreamJob: map 100% reduce 0% [...] INFO streaming.StreamJob: map 100% reduce 33% [...] INFO streaming.StreamJob: map 100% reduce 70% [...] INFO streaming.StreamJob: map 100% reduce 77% [...] INFO streaming.StreamJob: map 100% reduce 100% [...] INFO streaming.StreamJob: Job complete: job_200803031615_0021 [...] INFO streaming.StreamJob: Output: /user/hduser/gutenberg-output user@ubuntu:/usr/local/hadoop$ 查看执行结果 上面的信息告诉我们任务执行成功了，结果文件存储在hdfs上的/user/hduser/gutenberg-output目录下，我们来看一眼(是的，HDFS小抄又可以用上了)。\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls /user/hduser/gutenberg-output Found 1 items /user/hduser/gutenberg-output/part-00000 \u0026amp;lt;r 1\u0026amp;gt; 903193 2017-03-21 13:00 user@ubuntu:/usr/local/hadoop$ 还可以直接查看结果的内容：\nuser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -cat /user/hduser/gutenberg-output/part-00000 \u0026quot;(Lo)cra\u0026quot; 1 \u0026quot;1490 1 \u0026quot;1498,\u0026quot; 1 \u0026quot;35\u0026quot; 1 \u0026quot;40,\u0026quot; 1 \u0026quot;A 2 \u0026quot;AS-IS\u0026quot;. 2 \u0026quot;A_ 1 \u0026quot;Absoluti 1 [...] user@ubuntu:/usr/local/hadoop$ "
},
{
	"uri": "https://lczen.github.io/en/posts/spark/",
	"title": "Spark",
	"tags": ["spark"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "Spark 核心概念与操作 spark简介 Apache Spark是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：\n 通用计算引擎 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架； 基于内存 数据可缓存在内存中，特别适用于需要迭代多次运算的场景； 与Hadoop集成 能够直接读写HDFS中的数据，并能运行在YARN之上。  Spark是用Scala语言编写的，所提供的API也很好地利用了这门语言的特性，当然作为数据科学的一环，它也可以使用Java和Python编写应用。这里我们将用Python给大家做讲解。\nspark核心 Spark支持多种运行模式。单机部署下，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据实际情况选择Spark自带的独立（Standalone）运行模式、YARN运行模式或者Mesos模式。虽然模式多，但是Spark的运行架构基本由三部分组成，包括SparkContext（驱动程序）、ClusterManager（集群资源管理器）和Executor（任务执行进程）。　 SparkContext提交作业，向ClusterManager申请资源； ClusterManager会根据当前集群的资源使用情况，进行有条件的FIFO策略：先分配的应用程序尽可能多地获取资源，后分配的应用程序则在剩余资源中筛选，没有合适资源的应用程序只能等待其他应用程序释放资源； ClusterManager默认情况下会将应用程序分布在尽可能多的Worker上，这种分配算法有利于充分利用集群资源，适合内存使用多的场景，以便更好地做到数据处理的本地性；另一种则是分布在尽可能少的Worker上，这种适合CPU密集型且内存使用较少的场景； Excutor创建后与SparkContext保持通讯，SparkContext分配任务集给Excutor，Excutor按照一定的调度策略执行任务集。  Spark包含1个driver(笔记本电脑或者集群网关机器上)和若干个executor(在各个节点上)，通过**SparkContext(简称sc)**连接Spark集群、创建RDD、累加器（accumlator）、广播变量（broadcast variables），简单可以认为SparkContext（驱动程序）是Spark程序的根本。\nDriver会把计算任务分成一系列小的task，然后送到executor执行。executor之间可以通信，在每个executor完成自己的task以后，所有的信息会被传回。\nRDD(弹性分布式数据集)介绍 在Spark里，所有的处理和计算任务都会被组织成一系列Resilient Distributed Dataset(弹性分布式数据集，简称RDD)上的transformations(转换) 和 actions(动作)。\nRDD是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合，可以将RDD持久化到内存中，这样就可以有效地在并行操作中复用（在机器学习这种需要反复迭代的任务中非常有效）。在节点发生错误时RDD也可以自动恢复。\n说起来，RDD就像一个NumPy array或者一个Pandas Series，可以视作一个有序的item集合。\n只不过这些item并不存在driver端的内存里，而是被分割成很多个partitions，每个partition的数据存在集群的executor的内存中。\nRDD是最重要的载体，我们看看如何初始化这么一个对象:\n初始化RDD方法1 如果你本地内存中已经有一份序列数据(比如python的list)，你可以通过sc.parallelize去初始化一个RDD\n当你执行这个操作以后，list中的元素将被自动分块(partitioned)，并且把每一块送到集群上的不同机器上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import\tfindspark findspark.init() import pyspark from pyspark import SparkContext from pyspark import SparkConf conf=SparkConf().setAppName(\u0026#34;miniProject\u0026#34;).setMaster(\u0026#34;local[*]\u0026#34;) sc=SparkContext.getOrCreate(conf) my_list = [1,2,3,4,5] #存放在当前环境内存当中的list rdd = sc.parallelize(my_list) rdd rdd.getNumPartitions()   如果你想看看分区状况怎么办\n1  rdd.glom().collect()#[[], [1], [], [2], [3], [], [4], [5]]   在这个例子中，是一个4-core的CPU笔记本\nSpark创建了4个executor，然后把数据分成4个块。\nTips：使用sc.parallelize，你可以把Python list，NumPy array或者Pandas Series、Pandas DataFrame转成Spark RDD。\n初始化RDD方法2 第2种方式当然是直接把文本读到RDD了\n你的每一行都会被当做一个item，不过需要注意的一点是，Spark一般默认你的路径是指向HDFS的，如果你要从本地读取文件的话，给一个file://开头的全局路径。\n!head -5 ../data/yob1880.txt\n1 2 3 4  # Record current path for future use import os cwd = os.getcwd() cwd#\u0026#39;D:\\\\新建文件夹 (3)\\\\spark在线环境\\\\course2\\\\lab\u0026#39;   1 2 3 4  # File from Pandas exercises rdd = sc.textFile(\u0026#34;../data/yob1880.txt\u0026#34;) rdd.first() rdd = sc.wholeTextFiles(\u0026#34;../data/yob1880.txt\u0026#34;)   其余初始化RDD的方法 RDD还可以通过其他的方式初始化，包括\n HDFS上的文件 Hive中的数据库与表 Spark SQL得到的结果  后面会提到这个部分\nRDD transformations和actions 大家还对python的list comprehension有印象吗，RDDs可以进行一系列的变换得到新的RDD，有点类似那个过程，我们先给大家提一下RDD上最最常用到的transformation:\n map() 对RDD的每一个item都执行同一个操作 flatMap() 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list filter() 筛选出来满足条件的item distinct() 对RDD中的item去重 sample() 从RDD中的item中采样一部分出来，有放回或者无放回 sortBy() 对RDD中的item进行排序\n如果你想看操作后的结果，可以用一个叫做collect()的action把所有的item转成一个Python list。\n简单的例子如下:  1 2 3 4 5 6 7 8  numbersRDD = sc.parallelize(range(1,10+1)) print(numbersRDD.collect()) squaresRDD = numbersRDD.map(lambda x: x**2) # 1进1出 print(squaresRDD.collect()) filteredRDD = numbersRDD.filter(lambda x: x % 2 == 0) # Only the evens print(filteredRDD.collect())   然后咱们看看flatMap()的平展功能:\n1 2 3 4  sentencesRDD = sc.parallelize([\u0026#39;Hello world\u0026#39;, \u0026#39;My name is Patrick\u0026#39;]) wordsRDD = sentencesRDD.flatMap(lambda sentence: sentence.split(\u0026#34;\u0026#34;)) print(wordsRDD.collect()) print(wordsRDD.count())   [\u0026lsquo;Hello\u0026rsquo;, \u0026lsquo;world\u0026rsquo;, \u0026lsquo;My\u0026rsquo;, \u0026lsquo;name\u0026rsquo;, \u0026lsquo;is\u0026rsquo;, \u0026lsquo;Patrick\u0026rsquo;]\n6\n为了做一个小小的对应，咱们看看python里对应的操作大概是什么样的:\n1 2 3 4 5  l = [\u0026#39;Hello world\u0026#39;, \u0026#39;My name is Patrick\u0026#39;] ll = [] for sentence in l: ll = ll + sentence.split(\u0026#34;\u0026#34;) ll   [\u0026lsquo;Hello\u0026rsquo;, \u0026lsquo;world\u0026rsquo;, \u0026lsquo;My\u0026rsquo;, \u0026lsquo;name\u0026rsquo;, \u0026lsquo;is\u0026rsquo;, \u0026lsquo;Patrick\u0026rsquo;]\n比较炫酷的是，前面提到的Transformation，可以一个接一个地串联，比如:\n1 2 3 4 5 6 7 8 9 10 11 12 13  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] def doubleIfOdd(x): if x % 2 == 1: return 2 * x else: return x resultRDD = (numbersRDD # In parentheses so we can write each .map(doubleIfOdd) # transformation in one line .filter(lambda x: x \u0026gt; 6) .distinct()) resultRDD.collect()   [8, 10, 18, 14]\nRDD间的操作 如果你手头上有2个RDD了，下面的这些操作能够帮你对他们以个种方式组合得到1个RDD:\n rdd1.union(rdd2): 所有rdd1和rdd2中的item组合 rdd1.intersection(rdd2): rdd1 和 rdd2的交集 rdd1.substract(rdd2): 所有在rdd1中但不在rdd2中的item（差集） **rdd1.cartesian(rdd2): rdd1 和 rdd2中所有的元素笛卡尔乘积 **\n简单的例子如下:  1 2 3  numbersRDD = sc.parallelize([1,2,3]) moreNumbersRDD = sc.parallelize([2,3,4]) numbersRDD.union(moreNumbersRDD).collect()   [1, 2, 3, 2, 3, 4]\n1 2 3  numbersRDD.intersection(moreNumbersRDD).collect() numbersRDD.subtract(moreNumbersRDD).collect() numbersRDD.cartesian(moreNumbersRDD).collect()   特别注意：Spark的一个核心概念是惰性计算。当你把一个RDD转换成另一个的时候，这个转换不会立即生效执行！！！\nSpark会把它先记在心里，等到真的需要拿到转换结果的时候，才会重新组织你的transformations(因为可能有一连串的变换)\n这样可以避免不必要的中间结果存储和通信。\n刚才提到了惰性计算，那么什么东西能让它真的执行转换与运算呢？\n是的，就是我们马上提到的Actions，下面是常见的action，当他们出现的时候，表明我们需要执行刚才定义的transform了:\n collect(): 计算所有的items并返回所有的结果到driver端，接着 collect()会以Python list的形式返回结果 first(): 和上面是类似的，不过只返回第1个item take(n): 类似，但是返回n个item count(): 计算RDD中item的个数 top(n): 返回头n个items，按照自然结果排序 reduce(): 对RDD中的items做聚合\n我们之前已经看过 collect(), first() 和 count() 的例子了。 咱们看看 reduce() 如何使用。比如Spark里从1加到10你可以这么做。  1 2 3  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] rdd = sc.parallelize(range(1,10+1)) rdd.reduce(lambda x, y: x + y)   如果你想了解一下reduce的细节的话，其实可能会先在每个分区(partition)里完成reduce操作，然后再全局地进行reduce。\n这个过程你可以从如下的代码大致理解。\n1 2 3 4 5  def f(x,y): return x + y l = [1,2,3,4] f(f(f(l[0],l[1]), l[2]), l[3])   有一个很有用的操作，我们试想一下，有时候我们需要重复用到某个transform序列得到的RDD结果。但是一遍遍重复计算显然是要开销的，所以我们可以通过一个叫做cache()的操作把它暂时地存储在内存中:\n1 2 3 4 5 6 7 8 9  # Calculate the average of all the squares from 1 to 10 import numpy as np numbersRDD = sc.parallelize(np.linspace(1.0, 10.0, 10)) squaresRDD = numbersRDD.map(lambda x: x**2) squaresRDD.cache() # Preserve the actual items of this RDD in memory avg = squaresRDD.reduce(lambda x, y: x + y) / squaresRDD.count() print(avg)   缓存RDD结果对于重复迭代的操作非常有用，比如很多机器学习的算法，训练过程需要重复迭代。\n1 2 3 4 5 6 7 8 9  ## 针对更复杂结构的transformations和actions 咱们刚才已经见识到了`Spark`中最常见的transform和action，但是有时候我们会遇到更复杂的结构，比如非常非常经典的是以元组形式组织的k-v对（key, value）\u0026lt;br\u0026gt; 我们把它叫做**pair RDDs**，而Sark中针对这种item结构的数据，定义了一些transformation和action: * `reduceByKey()`: 对所有有着相同key的items执行reduce操作 * `groupByKey()`: 返回类似(key, listOfValues)元组的RDD，后面的value List 是同一个key下面的 * `sortByKey()`: 按照key排序 * `countByKey()`: 按照key去对item个数进行统计 * `collectAsMap()`: 和collect有些类似，但是返回的是k-v的字典   以下是Spark中的一些例子\n怎么说呢，统计这个案例算是分布式（hadoop/spark）相关知识中的“Hello World”\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  rdd = sc.parallelize([\u0026#34;Hello hello\u0026#34;, \u0026#34;Hello New York\u0026#34;, \u0026#34;York says hello\u0026#34;]) resultRDD = ( rdd .flatMap(lambda sentence: sentence.split(\u0026#34;\u0026#34;)) # split into words .map(lambda word: word.lower()) # lowercase .map(lambda word: (word, 1)) # count each appearance .reduceByKey(lambda x, y: x + y) # add counts for each word ) resultRDD.collect() [(\u0026#39;hello\u0026#39;, 4), (\u0026#39;york\u0026#39;, 2), (\u0026#39;new\u0026#39;, 1), (\u0026#39;says\u0026#39;, 1)] 我们将结果以k-v字典的形式返回 ```pythonresult = resultRDD.collectAsMap() result {\u0026#39;hello\u0026#39;: 4, \u0026#39;york\u0026#39;: 2, \u0026#39;new\u0026#39;: 1, \u0026#39;says\u0026#39;: 1}   如果你想要出现频次最高的2个词，可以这么做:\n1 2 3 4  print(resultRDD .sortBy(keyfunc=lambda x: x[1], ascending=False) .take(2)) [(\u0026#39;hello\u0026#39;, 4), (\u0026#39;york\u0026#39;, 2)]   还有一个很有意思的操作是，在给定2个pairRDD后，我们可以通过一个类似SQL的方式去join他们。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Home of different people homesRDD = sc.parallelize([ (\u0026#39;Brussels\u0026#39;, \u0026#39;John\u0026#39;), (\u0026#39;Brussels\u0026#39;, \u0026#39;Jack\u0026#39;), (\u0026#39;Leuven\u0026#39;, \u0026#39;Jane\u0026#39;), (\u0026#39;Antwerp\u0026#39;, \u0026#39;Jill\u0026#39;), ]) # Quality of life index for various cities lifeQualityRDD = sc.parallelize([ (\u0026#39;Brussels\u0026#39;, 10), (\u0026#39;Antwerp\u0026#39;, 7), (\u0026#39;RestOfFlanders\u0026#39;, 5), ]) homesRDD.join(lifeQualityRDD).collect() [(\u0026#39;Brussels\u0026#39;, (\u0026#39;John\u0026#39;, 10)), (\u0026#39;Brussels\u0026#39;, (\u0026#39;Jack\u0026#39;, 10)), (\u0026#39;Antwerp\u0026#39;, (\u0026#39;Jill\u0026#39;, 7))]   1 2 3 4 5  homesRDD.leftOuterJoin(lifeQualityRDD).collect() [(\u0026#39;Brussels\u0026#39;, (\u0026#39;John\u0026#39;, 10)), (\u0026#39;Brussels\u0026#39;, (\u0026#39;Jack\u0026#39;, 10)), (\u0026#39;Antwerp\u0026#39;, (\u0026#39;Jill\u0026#39;, 7)), (\u0026#39;Leuven\u0026#39;, (\u0026#39;Jane\u0026#39;, None))]   1 2 3 4 5  homesRDD.rightOuterJoin(lifeQualityRDD).collect() [(\u0026#39;Brussels\u0026#39;, (\u0026#39;John\u0026#39;, 10)), (\u0026#39;Brussels\u0026#39;, (\u0026#39;Jack\u0026#39;, 10)), (\u0026#39;Antwerp\u0026#39;, (\u0026#39;Jill\u0026#39;, 7)), (\u0026#39;RestOfFlanders\u0026#39;, (None, 5))]   1 2 3 4 5 6 7 8 9 10 11 12 13 14  homesRDD.cogroup(lifeQualityRDD).collect() [(\u0026#39;Brussels\u0026#39;, (\u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30358\u0026gt;, \u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a309e8\u0026gt;)), (\u0026#39;Antwerp\u0026#39;, (\u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30e80\u0026gt;, \u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30ac8\u0026gt;)), (\u0026#39;RestOfFlanders\u0026#39;, (\u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30b70\u0026gt;, \u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30f28\u0026gt;)), (\u0026#39;Leuven\u0026#39;, (\u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a302b0\u0026gt;, \u0026lt;pyspark.resultiterable.ResultIterable at 0x13ff8a30fd0\u0026gt;))]   1 2 3 4 5 6 7 8 9 10 11 12 13  # Oops! Those \u0026lt;ResultIterable\u0026gt;s are Spark\u0026#39;s way of returning a list # that we can walk over, without materializing the list. # Let\u0026#39;s materialize the lists to make the above more readable: (homesRDD .cogroup(lifeQualityRDD) .map(lambda x: (x[0], (list(x[1][0]), list(x[1][1])))) .collect()) [(\u0026#39;Brussels\u0026#39;, ([\u0026#39;John\u0026#39;, \u0026#39;Jack\u0026#39;], [10])), (\u0026#39;Antwerp\u0026#39;, ([\u0026#39;Jill\u0026#39;], [7])), (\u0026#39;RestOfFlanders\u0026#39;, ([], [5])), (\u0026#39;Leuven\u0026#39;, ([\u0026#39;Jane\u0026#39;], []))]   Spark DataFrame 总览 Spark SQL 是 Spark 处理结构化数据的一个模块, 与基础的 Spark RDD API 不同, Spark SQL 提供了查询结构化数据及计算结果等信息的接口.在内部, Spark SQL 使用这个额外的信息去执行额外的优化.有几种方式可以跟 Spark SQL 进行交互, 包括 SQL 和 Dataset API.当使用相同执行引擎进行计算时, 无论使用哪种 API / 语言都可以快速的计算。\nSQL Spark SQL 的功能之一是执行 SQL 查询，Spark SQL 也能够被用于从已存在的 Hive 环境中读取数据。当以另外的编程语言运行SQL 时, 查询结果将以 Dataset/DataFrame的形式返回，也可以使用 命令行或者通过 JDBC/ODBC与 SQL 接口交互.\nDataFrames 从RDD里可以生成类似大家在pandas中的DataFrame，同时可以方便地在上面完成各种操作。\n构建SparkSession Spark SQL中所有功能的入口点是 SparkSession 类. 要创建一个 SparkSession, 仅使用 SparkSession.builder()就可以了:\n1 2 3 4 5 6 7 8 9 10  import\tfindspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .appName(\u0026#34;Python Spark SQL\u0026#34;) \\ .config(\u0026#34;spark.some.config.option\u0026#34;, \u0026#34;some-value\u0026#34;) \\ .getOrCreate()   创建 DataFrames 在一个 SparkSession中, 应用程序可以从一个 已经存在的 RDD 或者 hive表, 或者从Spark数据源中创建一个DataFrames.\n举个例子, 下面就是基于一个JSON文件创建一个DataFrame:\n1 2 3 4 5  !cat ../data/people.json {\u0026#34;name\u0026#34;:\u0026#34;Michael\u0026#34;, \u0026#34;age\u0026#34;:33} {\u0026#34;name\u0026#34;:\u0026#34;Andy\u0026#34;, \u0026#34;age\u0026#34;:30} {\u0026#34;name\u0026#34;:\u0026#34;Justin\u0026#34;, \u0026#34;age\u0026#34;:19}   1 2 3  df = spark.read.json(\u0026#34;../data/people.json\u0026#34;) df # DataFrame[age: bigint, name: string] df.show()   +---+-------+ |age| name| +---+-------+ | 33|Michael| | 30| Andy| | 19| Justin| +---+-------+ DataFrame 操作 DataFrames 提供了一个特定的语法用在 Scala, Java, Python and R中机构化数据的操作。\n在Python中，可以通过(df.age) 或者(df[\u0026lsquo;age\u0026rsquo;])来获取DataFrame的列. 虽然前者便于交互式操作, 但是还是建议用户使用后者, 这样不会破坏列名，也能引用DataFrame的类。\n注意以下操作的select 1  df.printSchema()   root |-- age: long (nullable = true) |-- name: string (nullable = true) 1  df.select(\u0026#34;name\u0026#34;).show()   +-------+ | name| +-------+ |Michael| | Andy| | Justin| +-------+ 1  df.select([\u0026#34;name\u0026#34;,\u0026#39;age\u0026#39;]).show()   +-------+---+ | name|age| +-------+---+ |Michael| 33| | Andy| 30| | Justin| 19| +-------+---+ 1  df.select(df[\u0026#39;name\u0026#39;], df[\u0026#39;age\u0026#39;] + 1).show()   +-------+---------+ | name|(age + 1)| +-------+---------+ |Michael| 34| | Andy| 31| | Justin| 20| +-------+---------+ 以下操作的filter做数据选择 1  df.filter(df[\u0026#39;age\u0026#39;] \u0026gt; 21).show()   +---+-------+ |age| name| +---+-------+ | 33|Michael| | 30| Andy| +---+-------+ 1  df.groupBy(\u0026#34;age\u0026#34;).count().show()   +---+-----+ |age|count| +---+-----+ | 19| 1| | 33| 1| | 30| 1| +---+-----+ spark SQL SparkSession 的 sql 函数可以让应用程序以编程的方式运行 SQL 查询, 并将结果作为一个 DataFrame 返回.\n1 2 3 4  df.createOrReplaceTempView(\u0026#34;people\u0026#34;) sqlDF = spark.sql(\u0026#34;SELECT * FROM people\u0026#34;) sqlDF.show()   +---+-------+ |age| name| +---+-------+ | 33|Michael| | 30| Andy| | 19| Justin| +---+-------+ spark DataFrame与RDD交互 Spark SQL 支持两种不同的方法用于转换已存在的 RDD 成为 Dataset\n第一种方法是使用反射去推断一个包含指定的对象类型的 RDD 的 Schema.在你的 Spark 应用程序中当你已知 Schema 时这个基于方法的反射可以让你的代码更简洁.\n第二种用于创建 Dataset 的方法是通过一个允许你构造一个 Schema 然后把它应用到一个已存在的 RDD 的编程接口.然而这种方法更繁琐, 当列和它们的类型知道运行时都是未知时它允许你去构造 Dataset.\n!cat ../data/people.txt\n反射推断 1 2 3 4 5 6 7 8 9 10  from pyspark.sql import Row sc = spark.sparkContext lines = sc.textFile(\u0026#34;../data/people.txt\u0026#34;) parts = lines.map(lambda l: l.split(\u0026#34;,\u0026#34;)) people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # Infer the schema, and register the DataFrame as a table. schemaPeople = spark.createDataFrame(people) schemaPeople.createOrReplaceTempView(\u0026#34;people\u0026#34;)   1  teenagers = spark.sql(\u0026#34;SELECT name FROM people WHERE age \u0026gt;= 13 AND age \u0026lt;= 19\u0026#34;)   1  type(teenagers)   pyspark.sql.dataframe.DataFrame\n1  type(teenagers.rdd)   pyspark.rdd.RDD\n1  teenagers.rdd.first()   Row(name='Justin\u0026rsquo;)\n1 2 3  teenNames = teenagers.rdd.map(lambda p: \u0026#34;Name: \u0026#34; + p.name).collect() for name in teenNames: print(name)   Name: Justin\n以编程的方式指定Schema 也可以通过以下的方式去初始化一个 DataFrame。\n RDD从原始的RDD穿件一个RDD的toples或者一个列表; Step 1 被创建后, 创建 Schema 表示一个 StructType 匹配 RDD 中的结构. 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD .  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from pyspark.sql.types import * sc = spark.sparkContext # Load a text file and convert each line to a Row. lines = sc.textFile(\u0026#34;../data/people.txt\u0026#34;) parts = lines.map(lambda l: l.split(\u0026#34;,\u0026#34;)) # Each line is converted to a tuple. people = parts.map(lambda p: (p[0], p[1].strip())) # The schema is encoded in a string. schemaString = \u0026#34;name age\u0026#34; fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()] schema = StructType(fields) # Apply the schema to the RDD. schemaPeople = spark.createDataFrame(people, schema)   1 2 3  schemaPeople.createOrReplaceTempView(\u0026#34;people\u0026#34;) results = spark.sql(\u0026#34;SELECT name FROM people\u0026#34;) results.show()   +-------+ | name| +-------+ |Michael| | Andy| | Justin| +-------+ Spark DataFrame vs SQL Spark DataFrame vs SQL 的小练习 a.初始化Spark Session 1 2  import\tfindspark findspark.init()   1 2 3 4 5 6 7  from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .appName(\u0026#34;Python Spark SQL\u0026#34;) \\ .config(\u0026#34;spark.some.config.option\u0026#34;, \u0026#34;some-value\u0026#34;) \\ .getOrCreate()   b.构建数据集与序列化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  stringJSONRDD = spark.sparkContext.parallelize((\u0026#34;\u0026#34;\u0026#34;{ \u0026#34;id\u0026#34;: \u0026#34;123\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;Katie\u0026#34;,\u0026#34;age\u0026#34;: 19,\u0026#34;eyeColor\u0026#34;: \u0026#34;brown\u0026#34;}\u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34;{\u0026#34;id\u0026#34;: \u0026#34;234\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;Michael\u0026#34;,\u0026#34;age\u0026#34;: 22,\u0026#34;eyeColor\u0026#34;: \u0026#34;green\u0026#34;}\u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34;{\u0026#34;id\u0026#34;: \u0026#34;345\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;Simone\u0026#34;,\u0026#34;age\u0026#34;: 23,\u0026#34;eyeColor\u0026#34;: \u0026#34;blue\u0026#34;}\u0026#34;\u0026#34;\u0026#34;) )   1 2  # 构建DataFrame swimmersJSON = spark.read.json(stringJSONRDD)   1 2  # 创建临时表 swimmersJSON.createOrReplaceTempView(\u0026#34;swimmersJSON\u0026#34;)   1 2  # DataFrame信息 swimmersJSON.show()   +---+--------+---+-------+ |age|eyeColor| id| name| +---+--------+---+-------+ | 19| brown|123| Katie| | 22| green|234|Michael| | 23| blue|345| Simone| +---+--------+---+-------+ 1  spark.sql(\u0026#34;select * from swimmersJSON\u0026#34;).show()   +---+--------+---+-------+ |age|eyeColor| id| name| +---+--------+---+-------+ | 19| brown|123| Katie| | 22| green|234|Michael| | 23| blue|345| Simone| +---+--------+---+-------+ 1 2  # 执行SQL请求 spark.sql(\u0026#34;select * from swimmersJSON\u0026#34;).collect()   [Row(age=19, eyeColor='brown', id='123', name='Katie'), Row(age=22, eyeColor='green', id='234', name='Michael'), Row(age=23, eyeColor='blue', id='345', name='Simone')] 1 2  # 输出数据表的格式 swimmersJSON.printSchema()   |-- age: long (nullable = true) |-- eyeColor: string (nullable = true) |-- id: string (nullable = true) |-- name: string (nullable = true) 1 2  # 执行SQL spark.sql(\u0026#34;select count(1) from swimmersJSON\u0026#34;)   DataFrame[count(1): bigint]\n1  spark.sql(\u0026#34;select count(1) from swimmersJSON\u0026#34;).show()   +\u0026mdash;\u0026mdash;\u0026ndash;+\n|count(1)|\n+\u0026mdash;\u0026mdash;\u0026ndash;+\n| 3|\n+\u0026mdash;\u0026mdash;\u0026ndash;+\nc.DataFrame的请求方式 vs SQL的写法 1 2  # DataFrame的写法 swimmersJSON.select(\u0026#34;id\u0026#34;, \u0026#34;age\u0026#34;).filter(\u0026#34;age = 22\u0026#34;).show()   +\u0026mdash;+\u0026mdash;+\n| id|age|\n+\u0026mdash;+\u0026mdash;+\n|234| 22|\n+\u0026mdash;+\u0026mdash;+\n1 2  # SQL的写法 spark.sql(\u0026#34;select id, age from swimmersJSON where age = 22\u0026#34;).show()   +\u0026mdash;+\u0026mdash;+\n| id|age|\n+\u0026mdash;+\u0026mdash;+\n|234| 22|\n+\u0026mdash;+\u0026mdash;+\n1 2  # DataFrame的写法 swimmersJSON.select(\u0026#34;name\u0026#34;, \u0026#34;eyeColor\u0026#34;).filter(\u0026#34;eyeColor like \u0026#39;b%\u0026#39;\u0026#34;).show()   +\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n| name|eyeColor|\n+\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n| Katie| brown|\n|Simone| blue|\n+\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n1 2  # SQL的写法 spark.sql(\u0026#34;select name, eyeColor from swimmersJSON where eyeColor like \u0026#39;b%\u0026#39;\u0026#34;).show()   +\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n| name|eyeColor|\n+\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n| Katie| brown|\n|Simone| blue|\n+\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026ndash;+\n"
},
{
	"uri": "https://lczen.github.io/en/tags/spark/",
	"title": "spark",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/",
	"title": "大数据处理",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/",
	"title": "数据分析",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/tags/%E7%BB%98%E5%9B%BE/",
	"title": "绘图",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/plotfigure/",
	"title": "绘图",
	"tags": ["绘图"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "seaborn 基于pandas的内置可视化 基本绘图 Series和DataFrame上的这个功能只是使用matplotlib库的plot()方法的简单包装实现。\n1 2 3 4 5 6 7  %matplotlib inline import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(10,4),index=pd.date_range(\u0026#39;2019/04/15\u0026#39;, periods=10), columns=list(\u0026#39;ABCD\u0026#39;)) df.plot()   如果索引由日期组成，则调用gct().autofmt_xdate()来格式化x轴，如上图所示。\n我们可以使用x和y关键字绘制一列与另一列。\n绘图方法允许除默认线图之外的少数绘图样式。 这些方法可以作为plot()的kind关键字参数提供。这些包括 -\n bar或barh为条形 hist为直方图 boxplot为盒型图 area为“面积” scatter为散点图  条形图 1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(np.random.rand(10,4),columns=[\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;]) df.plot.bar()   要生成一个堆积条形图，通过指定：pass stacked=True\n1 2 3  import pandas as pd df = pd.DataFrame(np.random.rand(10,4),columns=[\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;]) df.plot.bar(stacked=True)   要获得水平条形图，使用barh()方法\n1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(np.random.rand(10,4),columns=[\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;]) df.plot.barh(stacked=True)   直方图 可以使用plot.hist()方法绘制直方图。我们可以指定bins的数量值。\n1 2 3 4 5  import pandas as pd import numpy as np df = pd.DataFrame({\u0026#39;a\u0026#39;:np.random.randn(1000)+1,\u0026#39;b\u0026#39;:np.random.randn(1000),\u0026#39;c\u0026#39;: np.random.randn(1000) - 1}, columns=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) df.plot.hist(bins=20)   要为每列绘制不同的直方图，要用df.plot()\n1 2 3 4 5  import pandas as pd import numpy as np df=pd.DataFrame({\u0026#39;a\u0026#39;:np.random.randn(1000)+1,\u0026#39;b\u0026#39;:np.random.randn(1000),\u0026#39;c\u0026#39;: np.random.randn(1000) - 1}, columns=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) df.hist(bins=20)   箱形图 Boxplot可以绘制调用Series.box.plot()和DataFrame.box.plot()或DataFrame.boxplot()来可视化每列中值的分布。\n例如，这里是一个箱形图，表示对[0,1)上的统一随机变量的10次观察的五次试验。\n1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(np.random.rand(10, 5), columns=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;]) df.plot.box()   75%分位数,中位数,25%分位数;(75%-25%)*1.5作为边界值\n区域块图 可以使用Series.plot.area()或DataFrame.plot.area()方法创建区域图形\n1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(np.random.rand(10, 4), columns=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]) df.plot.area()   散点图 可以使用DataFrame.plot.scatter()方法创建散点图\n1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(np.random.rand(50, 4), columns=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]) df.plot.scatter(x=\u0026#39;a\u0026#39;, y=\u0026#39;b\u0026#39;)   饼状图 饼状图可以使用DataFrame.plot.pie()方法创建\n1 2 3 4  import pandas as pd import numpy as np df = pd.DataFrame(3 * np.random.rand(4), index=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;], columns=[\u0026#39;x\u0026#39;]) df.plot.pie(subplots=True)   Matplotlib可视化手册 1.matplotlib 安装配置 sudo pip install numpy sudo pip install scipy sudo pip install matplotlib 2.一幅可视化图的基本结构 通常，使用 numpy 组织数据, 使用 matplotlib API 进行数据图像绘制。 一幅数据图基本上包括如下结构：\n Data: 数据区，包括数据点、描绘形状 Axis: 坐标轴，包括 X 轴、 Y 轴及其标签、刻度尺及其标签 Title: 标题，数据图的描述 Legend: 图例，区分图中包含的多种曲线或不同分类的数据\n其他的还有图形文本 (Text)、注解 (Annotate)等其他描述\n  3.画法 下面以常规图为例，详细记录作图流程及技巧。按照绘图结构，可将数据图的绘制分为如下几个步骤：\n 导入 matplotlib 包相关工具包 准备数据，numpy 数组存储 绘制原始曲线 配置标题、坐标轴、刻度、图例 添加文字说明、注解 显示、保存绘图结果  3.1导包 会用到 matplotlib.pyplot、pylab 和 numpy\n1 2 3 4 5  #coding:utf-8 %matplotlib inline import numpy as np import matplotlib.pyplot as plt from pylab import *   3.2准备数据 numpy 常用来组织源数据:\n# 定义数据部分 x = np.arange(0., 10, 0.2) y1 = np.cos(x) y2 = np.sin(x) y3 = np.sqrt(x) #x = all_df['house_age'] #y = all_df['house_price'] 3.3绘制基本曲线 使用 plot 函数直接绘制上述函数曲线，可以通过配置 plot 函数参数调整曲线的样式、粗细、颜色、标记等：\n2.3.1 关于颜色的补充 主要是color参数：\n r 红色 g 绿色 b 蓝色 c cyan m 紫色 y 土黄色 k 黑色 w 白色  3.3.3 marker 参数 marker参数设定在曲线上标记的特殊符号，以区分不同的线段。常见的形状及表示符号如下图所示：\n3.4 设置坐标轴 可通过如下代码，移动坐标轴 spines\n1 2 3 4 5 6 7 8 9 10 11  # 坐标轴上移 ax = plt.subplot(111) #ax = plt.subplot(2,2,1) ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 去掉右边的边框线 ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 去掉上边的边框线 # 移动下边边框线，相当于移动 X 轴 ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) # 移动左边边框线，相当于移动 y 轴 ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.spines[\u0026#39;left\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0))   可通过如下代码，设置刻度尺间隔 lim、刻度标签 ticks\n1 2 3 4 5 6 7  # 设置 x, y 轴的刻度取值范围 plt.xlim(x.min()*1.1, x.max()*1.1) plt.ylim(-1.5, 4.0) # 设置 x, y 轴的刻度标签值 plt.xticks([2, 4, 6, 8, 10], [r\u0026#39;two\u0026#39;, r\u0026#39;four\u0026#39;, r\u0026#39;six\u0026#39;, r\u0026#39;8\u0026#39;, r\u0026#39;10\u0026#39;]) plt.yticks([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [r\u0026#39;-1.0\u0026#39;, r\u0026#39;0.0\u0026#39;, r\u0026#39;1.0\u0026#39;, r\u0026#39;2.0\u0026#39;, r\u0026#39;3.0\u0026#39;, r\u0026#39;4.0\u0026#39;])   可通过如下代码，设置 X、Y 坐标轴和标题：\n1 2 3 4  # 设置标题、x轴、y轴.labelpad控制刻度标注的上下位置 plt.title(r\u0026#39;$the \\function \\figure \\of \\cos(), \\sin() \\and \\sqrt()$\u0026#39;, fontsize=19) plt.xlabel(r\u0026#39;$the \\input \\value \\of \\x$\u0026#39;, fontsize=18, labelpad=88.8) plt.ylabel(r\u0026#39;$y = f(x)$\u0026#39;, fontsize=18, labelpad=12.5)   3.5 设置文字描述、注解 可通过如下代码，在数据图中添加文字描述 text：\n1 2 3  #0.8,0.9是取值范围label的坐标 plt.text(0.8, 0.9, r\u0026#39;$x \\in [0.0, \\10.0]$\u0026#39;, color=\u0026#39;k\u0026#39;, fontsize=15) plt.text(0.8, 0.8, r\u0026#39;$y \\in [-1.0, \\4.0]$\u0026#39;, color=\u0026#39;k\u0026#39;, fontsize=15)   可通过如下代码，在数据图中给特殊点添加注解 annotate：\n1 2 3  # 特殊点添加注解 plt.scatter([8,],[np.sqrt(8),], 50, color =\u0026#39;m\u0026#39;) # 使用散点图放大当前点 plt.annotate(r\u0026#39;$2\\sqrt{2}$\u0026#39;, xy=(8, np.sqrt(8)), xytext=(8.002, 2.83), fontsize=16, color=\u0026#39;#090909\u0026#39;, arrowprops=dict(arrowstyle=\u0026#39;-\u0026gt;\u0026#39;, connectionstyle=\u0026#39;arc3, rad=0.1\u0026#39;, color=\u0026#39;#090909\u0026#39;))   3.6 设置图例 可使用如下两种方式，给绘图设置图例：\n 1: 在 plt.plot 函数中添加 label 参数后，使用 plt.legend(loc=’up right’) 2: 不使用参数 label, 直接使用如下命令：\nplt.legend([\u0026lsquo;cos(x)', \u0026lsquo;sin(x)', \u0026lsquo;sqrt(x)'], loc='upper right\u0026rsquo;)\n  3.7 网格线开关 可使用如下代码，给绘图设置网格线：\n1 2  # 显示网格线 plt.grid(True)   3.8 显示与图像保存 1 2  plt.show() # 显示 savefig(\u0026#39;../figures/plot3d_ex.png\u0026#39;,dpi=48) # 保存，前提目录存在   4.完整的绘制程序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  #coding:utf-8 import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt from pylab import * # 定义数据部分 x = np.arange(0., 10, 0.2) y1 = np.cos(x) y2 = np.sin(x) y3 = np.sqrt(x) # 绘制 3 条函数曲线 plt.plot(x, y1, color=\u0026#39;blue\u0026#39;, linewidth=1.5, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;.\u0026#39;, label=r\u0026#39;$y = cos{x}$\u0026#39;) plt.plot(x, y2, color=\u0026#39;green\u0026#39;, linewidth=1.5, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;*\u0026#39;, label=r\u0026#39;$y = sin{x}$\u0026#39;) plt.plot(x, y3, color=\u0026#39;m\u0026#39;, linewidth=1.5, linestyle=\u0026#39;-\u0026#39;, marker=\u0026#39;x\u0026#39;, label=r\u0026#39;$y = \\sqrt{x}$\u0026#39;) # 坐标轴上移 ax = plt.subplot(111) ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 去掉右边的边框线 ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 去掉上边的边框线 # 移动下边边框线，相当于移动 X 轴 ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) # 移动左边边框线，相当于移动 y 轴 ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.spines[\u0026#39;left\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) # 设置 x, y 轴的取值范围 plt.xlim(x.min()*1.1, x.max()*1.1) plt.ylim(-1.5, 4.0) # 设置 x, y 轴的刻度值 plt.xticks([2, 4, 6, 8, 10], [r\u0026#39;2\u0026#39;, r\u0026#39;4\u0026#39;, r\u0026#39;6\u0026#39;, r\u0026#39;8\u0026#39;, r\u0026#39;10\u0026#39;]) plt.yticks([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [r\u0026#39;-1.0\u0026#39;, r\u0026#39;0.0\u0026#39;, r\u0026#39;1.0\u0026#39;, r\u0026#39;2.0\u0026#39;, r\u0026#39;3.0\u0026#39;, r\u0026#39;4.0\u0026#39;]) # 添加文字 plt.text(8, 1.5, r\u0026#39;$x \\in [0.0, \\10.0]$\u0026#39;, color=\u0026#39;k\u0026#39;, fontsize=15) plt.text(8, 1.8, r\u0026#39;$y \\in [-1.0, \\4.0]$\u0026#39;, color=\u0026#39;k\u0026#39;, fontsize=15) # 特殊点添加注解 plt.scatter([8,],[np.sqrt(8),], 50, color =\u0026#39;m\u0026#39;) # 使用散点图放大当前点 plt.annotate(r\u0026#39;$2\\sqrt{2}$\u0026#39;, xy=(8, np.sqrt(8)), xytext=(8.5, 2.2), fontsize=16, color=\u0026#39;#090909\u0026#39;, arrowprops=dict(arrowstyle=\u0026#39;-\u0026gt;\u0026#39;, connectionstyle=\u0026#39;arc3, rad=0.1\u0026#39;, color=\u0026#39;#090909\u0026#39;)) # 设置标题、x轴、y轴 plt.title(r\u0026#39;$the \\function \\figure \\of \\cos(), \\sin() \\and \\sqrt()$\u0026#39;, fontsize=19) plt.xlabel(r\u0026#39;$the \\input \\value \\of \\x$\u0026#39;, fontsize=18, labelpad=88.8) plt.ylabel(r\u0026#39;$y = f(x)$\u0026#39;, fontsize=18, labelpad=12.5) # 设置图例及位置 plt.legend(loc=\u0026#39;best\u0026#39;) # plt.legend([\u0026#39;cos(x)\u0026#39;, \u0026#39;sin(x)\u0026#39;, \u0026#39;sqrt(x)\u0026#39;], loc=\u0026#39;up right\u0026#39;) # 显示网格线 plt.grid(True) # 显示绘图 plt.show()   常用图形 细节看这里，看这里，看这里\n 曲线图：matplotlib.pyplot.plot(data) 灰度/直方图：matplotlib.pyplot.hist(data) 散点图：matplotlib.pyplot.scatter(data) 箱式/箱线图：matplotlib.pyplot.boxplot(data)  1 2 3  x = np.arange(-5,5,0.1) y = x ** 2 plt.plot(x,y)   1 2  x = np.random.normal(size=1000) plt.hist(x, bins=10)   1 2 3 4  plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (8,8) x = np.random.normal(size=1000) y = np.random.normal(size=1000) plt.scatter(x,y)   1 2 3  plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (8,8) x = np.random.normal(size=100) plt.boxplot(x)   箱式图(boxplot)科普\n 上边缘（Q3+1.5*IQR/“箱子高度”）、下边缘（Q1-1.5*IQR/“箱子高度”）、IQR=Q3-Q1 上四分位数（Q3）、下四分位数（Q1） 中位数 异常值 处理异常值时与3$\\sigma$标准的异同：统计边界是否受异常值影响、容忍度的大小\nmatplotlib的工具手册在这里可以查到  matplotlib应用案例 案例：自行车租赁数据分析与可视化 步骤1：导入数据与简易数据处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import pandas as pd # 读取数据到DataFrame import urllib # 获取网络数据 import shutil # 文件操作 import zipfile # 压缩解压 import os import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # 建立临时目录 try: os.system(\u0026#39;mkdir bike_data\u0026#39;) except: os.system(\u0026#39;rm -rf bike_data; mkdir bike_data\u0026#39;) data_source = \u0026#39;http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\u0026#39; # 网络数据地址 zipname = \u0026#39;bike_data/Bike-Sharing-Dataset.zip\u0026#39; # 拼接文件和路径 urllib.request.urlretrieve(data_source, zipname) # 获得数据 zip_ref = zipfile.ZipFile(zipname, \u0026#39;r\u0026#39;) # 创建一个ZipFile对象处理压缩文件 #zip_ref.extractall(temp_dir) # 解压 zip_ref.extractall(\u0026#39;bike_data\u0026#39;) zip_ref.close() daily_path = \u0026#39;bike_data/day.csv\u0026#39; daily_data = pd.read_csv(daily_path) # 读取csv文件 daily_data[\u0026#39;dteday\u0026#39;] = pd.to_datetime(daily_data[\u0026#39;dteday\u0026#39;]) # 把字符串数据传换成日期数据 drop_list = [\u0026#39;instant\u0026#39;, \u0026#39;season\u0026#39;, \u0026#39;yr\u0026#39;, \u0026#39;mnth\u0026#39;, \u0026#39;holiday\u0026#39;, \u0026#39;workingday\u0026#39;, \u0026#39;weathersit\u0026#39;, \u0026#39;atemp\u0026#39;, \u0026#39;hum\u0026#39;] # 不关注的列 daily_data.drop(drop_list, inplace = True, axis = 1) # inplace=true在对象上直接操作 daily_data.head() # 看一看数据~   dteday weekday temp windspeed casual registered cnt 0 2011-01-01 6 0.344167 0.160446 331 654 985 1 2011-01-02 0 0.363478 0.248539 131 670 801 2 2011-01-03 1 0.196364 0.248309 120 1229 1349 3 2011-01-04 2 0.200000 0.160296 108 1454 1562 4 2011-01-05 3 0.226957 0.186900 82 1518 1600 步骤2：配置参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  from __future__ import division, print_function # 引入3.x版本的除法和打印 from matplotlib import pyplot as plt import pandas as pd import numpy as np # 在notebook中显示绘图结果 %matplotlib inline # 设置一些全局的资源参数，可以进行个性化修改 import matplotlib # 设置图片尺寸 14\u0026#34; x 7\u0026#34; # rc: resource configuration matplotlib.rc(\u0026#39;figure\u0026#39;, figsize = (14, 7)) # 设置字体 14 matplotlib.rc(\u0026#39;font\u0026#39;, size = 14) # 不显示顶部和右侧的坐标线 matplotlib.rc(\u0026#39;axes.spines\u0026#39;, top = False, right = False) # 不显示网格 matplotlib.rc(\u0026#39;axes\u0026#39;, grid = False) # 设置背景颜色是白色 matplotlib.rc(\u0026#39;axes\u0026#39;, facecolor = \u0026#39;white\u0026#39;)   步骤3：关联分析 散点图  分析变量关系  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  from matplotlib import font_manager fontP = font_manager.FontProperties() fontP.set_family(\u0026#39;SimHei\u0026#39;) fontP.set_size(14) # 包装一个散点图的函数便于复用 def scatterplot(x_data, y_data, x_label, y_label, title): # 创建一个绘图对象 fig, ax = plt.subplots() # 设置数据、点的大小、点的颜色和透明度 ax.scatter(x_data, y_data, s = 10, color = \u0026#39;#539caf\u0026#39;, alpha = 0.75) # http://www.114la.com/other/rgb.htm # 添加标题和坐标说明 ax.set_title(title) ax.set_xlabel(x_label) ax.set_ylabel(y_label) # 绘制散点图 #温度上升，骑车的数量增多 scatterplot(x_data = daily_data[\u0026#39;temp\u0026#39;].values , y_data = daily_data[\u0026#39;cnt\u0026#39;].values , x_label = \u0026#39;Normalized temperature (C)\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Number of Check Outs vs Temperature\u0026#39;)   曲线图  拟合变量关系  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # 线性回归 import statsmodels.api as sm # 最小二乘 from statsmodels.stats.outliers_influence import summary_table # 获得汇总信息 x = sm.add_constant(daily_data[\u0026#39;temp\u0026#39;]) # 线性回归增加常数项 y=kx+b y = daily_data[\u0026#39;cnt\u0026#39;] regr = sm.OLS(y, x) # 普通最小二乘模型，ordinary least square model res = regr.fit() # 从模型获得拟合数据 st, data, ss2 = summary_table(res, alpha=0.05) # 置信水平alpha=5%，st数据汇总，data数据详情，ss2数据列名 fitted_values = data[:,2] # 包装曲线绘制函数 def lineplot(x_data, y_data, x_label, y_label, title): # 创建绘图对象 _, ax = plt.subplots() # 绘制拟合曲线，lw=linewidth，alpha=transparancy ax.plot(x_data, y_data, lw = 2, color = \u0026#39;#539caf\u0026#39;, alpha = 1) # 添加标题和坐标说明 ax.set_title(title) ax.set_xlabel(x_label) ax.set_ylabel(y_label) # 调用绘图函数 lineplot(x_data = daily_data[\u0026#39;temp\u0026#39;] , y_data = fitted_values , x_label = \u0026#39;Normalized temperature (C)\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Line of Best Fit for Number of Check Outs vs Temperature\u0026#39;)   带置信区间的曲线图  评估曲线拟合结果  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  # 获得5%置信区间的上下界 predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T # 创建置信区间DataFrame，上下界 CI_df = pd.DataFrame(columns = [\u0026#39;x_data\u0026#39;, \u0026#39;low_CI\u0026#39;, \u0026#39;upper_CI\u0026#39;]) CI_df[\u0026#39;x_data\u0026#39;] = daily_data[\u0026#39;temp\u0026#39;] CI_df[\u0026#39;low_CI\u0026#39;] = predict_mean_ci_low CI_df[\u0026#39;upper_CI\u0026#39;] = predict_mean_ci_upp CI_df.sort_values(\u0026#39;x_data\u0026#39;, inplace = True) # 根据x_data进行排序 # 绘制置信区间 def lineplotCI(x_data, y_data, sorted_x, low_CI, upper_CI, x_label, y_label, title): # 创建绘图对象 _, ax = plt.subplots() # 绘制预测曲线 ax.plot(x_data, y_data, lw = 1, color = \u0026#39;#539caf\u0026#39;, alpha = 1, label = \u0026#39;Fit\u0026#39;) # 绘制置信区间，顺序填充 ax.fill_between(sorted_x, low_CI, upper_CI, color = \u0026#39;#539caf\u0026#39;, alpha = 0.4, label = \u0026#39;95%CI\u0026#39;) # 添加标题和坐标说明 ax.set_title(title) ax.set_xlabel(x_label) ax.set_ylabel(y_label) # 显示图例，配合label参数，loc=“best”自适应方式 ax.legend(loc = \u0026#39;best\u0026#39;) # Call the function to create plot lineplotCI(x_data = daily_data[\u0026#39;temp\u0026#39;] , y_data = fitted_values , sorted_x = CI_df[\u0026#39;x_data\u0026#39;] , low_CI = CI_df[\u0026#39;low_CI\u0026#39;] , upper_CI = CI_df[\u0026#39;upper_CI\u0026#39;] , x_label = \u0026#39;Normalized temperature (C)\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Line of Best Fit for Number of Check Outs vs Temperature\u0026#39;)   双坐标曲线图  曲线拟合不满足置信阈值时，考虑增加独立变量 分析不同尺度多变量的关系  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # 双纵坐标绘图函数 def lineplot2y(x_data, x_label, y1_data, y1_color, y1_label, y2_data, y2_color, y2_label, title): _, ax1 = plt.subplots() ax1.plot(x_data, y1_data, color = y1_color) # 添加标题和坐标说明 ax1.set_ylabel(y1_label, color = y1_color) ax1.set_xlabel(x_label) ax1.set_title(title) ax2 = ax1.twinx() # 两个绘图对象共享横坐标轴 ax2.plot(x_data, y2_data, color = y2_color) ax2.set_ylabel(y2_label, color = y2_color) # 右侧坐标轴可见 ax2.spines[\u0026#39;right\u0026#39;].set_visible(True) # 调用绘图函数 lineplot2y(x_data = daily_data[\u0026#39;dteday\u0026#39;] , x_label = \u0026#39;Day\u0026#39; , y1_data = daily_data[\u0026#39;cnt\u0026#39;] , y1_color = \u0026#39;#539caf\u0026#39; , y1_label = \u0026#39;Check outs\u0026#39; , y2_data = daily_data[\u0026#39;windspeed\u0026#39;] , y2_color = \u0026#39;#7663b0\u0026#39; , y2_label = \u0026#39;Normalized windspeed\u0026#39; , title = \u0026#39;Check Outs and Windspeed Over Time\u0026#39;)   步骤4：分布分析 灰度图  粗略区间计数  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # 绘制灰度图的函数 def histogram(data, x_label, y_label, title): _, ax = plt.subplots() res = ax.hist(data, color = \u0026#39;#539caf\u0026#39;, bins=10) # 设置bin的数量 ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) return res # 绘图函数调用 res = histogram(data = daily_data[\u0026#39;registered\u0026#39;] , x_label = \u0026#39;Check outs\u0026#39; , y_label = \u0026#39;Frequency\u0026#39; , title = \u0026#39;Distribution of Registered Check Outs\u0026#39;) res[0] # value of bins res[1] # boundary of bins   堆叠直方图  比较两个分布  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # 绘制堆叠的直方图 def overlaid_histogram(data1, data1_name, data1_color, data2, data2_name, data2_color, x_label, y_label, title): # 归一化数据区间，对齐两个直方图的bins max_nbins = 10 #计算边界 data_range = [min(min(data1), min(data2)), max(max(data1), max(data2))] binwidth = (data_range[1] - data_range[0]) / max_nbins bins = np.arange(data_range[0], data_range[1] + binwidth, binwidth) # 生成直方图bins区间 # Create the plot _, ax = plt.subplots() ax.hist(data1, bins = bins, color = data1_color, alpha = 1, label = data1_name) ax.hist(data2, bins = bins, color = data2_color, alpha = 0.75, label = data2_name) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) ax.legend(loc = \u0026#39;best\u0026#39;) # Call the function to create plot overlaid_histogram(data1 = daily_data[\u0026#39;registered\u0026#39;] , data1_name = \u0026#39;Registered\u0026#39; , data1_color = \u0026#39;#539caf\u0026#39; , data2 = daily_data[\u0026#39;casual\u0026#39;] , data2_name = \u0026#39;Casual\u0026#39; , data2_color = \u0026#39;#7663b0\u0026#39; , x_label = \u0026#39;Check outs\u0026#39; , y_label = \u0026#39;Frequency\u0026#39; , title = \u0026#39;Distribution of Check Outs By Type\u0026#39;)    registered：注册的分布，正态分布，why casual：偶然的分布，疑似指数分布，why  密度图  精细刻画概率分布\nKDE: kernal density estimate\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # 计算概率密度 from scipy.stats import gaussian_kde data = daily_data[\u0026#39;registered\u0026#39;] density_est = gaussian_kde(data) # kernal density estimate: https://en.wikipedia.org/wiki/Kernel_density_estimation # 控制平滑程度，数值越大，越平滑 density_est.covariance_factor = lambda : .3 density_est._compute_covariance() x_data = np.arange(min(data), max(data), 200) # 绘制密度估计曲线 def densityplot(x_data, density_est, x_label, y_label, title): _, ax = plt.subplots() ax.plot(x_data, density_est(x_data), color = \u0026#39;#539caf\u0026#39;, lw = 2) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) # 调用绘图函数 densityplot(x_data = x_data , density_est = density_est , x_label = \u0026#39;Check outs\u0026#39; , y_label = \u0026#39;Frequency\u0026#39; , title = \u0026#39;Distribution of Registered Check Outs\u0026#39;)   1  type(density_est) #scipy.stats.kde.gaussian_kde   步骤5：组间分析  组间定量比较 分组粒度 组间聚类  柱状图  一级类间均值方差比较  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # 分天分析统计特征 mean_total_co_day = daily_data[[\u0026#39;weekday\u0026#39;, \u0026#39;cnt\u0026#39;]].groupby(\u0026#39;weekday\u0026#39;).agg([np.mean, np.std]) mean_total_co_day.columns = mean_total_co_day.columns.droplevel() # 定义绘制柱状图的函数 def barplot(x_data, y_data, error_data, x_label, y_label, title): _, ax = plt.subplots() # 柱状图 ax.bar(x_data, y_data, color = \u0026#39;#539caf\u0026#39;, align = \u0026#39;center\u0026#39;) # 绘制方差 # ls=\u0026#39;none\u0026#39;去掉bar之间的连线 ax.errorbar(x_data, y_data, yerr = error_data, color = \u0026#39;#297083\u0026#39;, ls = \u0026#39;none\u0026#39;, lw = 5) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) # 绘图函数调用 barplot(x_data = mean_total_co_day.index.values , y_data = mean_total_co_day[\u0026#39;mean\u0026#39;] , error_data = mean_total_co_day[\u0026#39;std\u0026#39;] , x_label = \u0026#39;Day of week\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Total Check Outs By Day of Week (0 = Sunday)\u0026#39;)   1 2  mean_total_co_day.columns daily_data[[\u0026#39;weekday\u0026#39;, \u0026#39;cnt\u0026#39;]].groupby(\u0026#39;weekday\u0026#39;).agg([np.mean, np.std])   \tcnt mean\tstd weekday\t0\t4228.828571\t1872.496629 1\t4338.123810\t1793.074013 2\t4510.663462\t1826.911642 3\t4548.538462\t2038.095884 4\t4667.259615\t1939.433317 5\t4690.288462\t1874.624870 6\t4550.542857\t2196.693009 堆积柱状图  多级类间相对占比比较  1 2  mean_by_reg_co_day = daily_data[[\u0026#39;weekday\u0026#39;, \u0026#39;registered\u0026#39;, \u0026#39;casual\u0026#39;]].groupby(\u0026#39;weekday\u0026#39;).mean() mean_by_reg_co_day   \tregistered\tcasual weekday\t0\t2890.533333\t1338.295238 1\t3663.990476\t674.133333 2\t3954.480769\t556.182692 3\t3997.394231\t551.144231 4\t4076.298077\t590.961538 5\t3938.000000\t752.288462 6\t3085.285714\t1465.257143 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # 分天统计注册和偶然使用的情况 mean_by_reg_co_day = daily_data[[\u0026#39;weekday\u0026#39;, \u0026#39;registered\u0026#39;, \u0026#39;casual\u0026#39;]].groupby(\u0026#39;weekday\u0026#39;).mean() # 分天统计注册和偶然使用的占比 mean_by_reg_co_day[\u0026#39;total\u0026#39;] = mean_by_reg_co_day[\u0026#39;registered\u0026#39;] + mean_by_reg_co_day[\u0026#39;casual\u0026#39;] mean_by_reg_co_day[\u0026#39;reg_prop\u0026#39;] = mean_by_reg_co_day[\u0026#39;registered\u0026#39;] / mean_by_reg_co_day[\u0026#39;total\u0026#39;] mean_by_reg_co_day[\u0026#39;casual_prop\u0026#39;] = mean_by_reg_co_day[\u0026#39;casual\u0026#39;] / mean_by_reg_co_day[\u0026#39;total\u0026#39;] # 绘制堆积柱状图 def stackedbarplot(x_data, y_data_list, y_data_names, colors, x_label, y_label, title): _, ax = plt.subplots() # 循环绘制堆积柱状图 for i in range(0, len(y_data_list)): if i == 0: ax.bar(x_data, y_data_list[i], color = colors[i], align = \u0026#39;center\u0026#39;, label = y_data_names[i]) else: # 采用堆积的方式，除了第一个分类，后面的分类都从前一个分类的柱状图接着画 # 用归一化保证最终累积结果为1 ax.bar(x_data, y_data_list[i], color = colors[i], bottom = y_data_list[i - 1], align = \u0026#39;center\u0026#39;, label = y_data_names[i]) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) ax.legend(loc = \u0026#39;upper right\u0026#39;) # 设定图例位置 # 调用绘图函数 stackedbarplot(x_data = mean_by_reg_co_day.index.values , y_data_list = [mean_by_reg_co_day[\u0026#39;reg_prop\u0026#39;], mean_by_reg_co_day[\u0026#39;casual_prop\u0026#39;]] , y_data_names = [\u0026#39;Registered\u0026#39;, \u0026#39;Casual\u0026#39;] , colors = [\u0026#39;#539caf\u0026#39;, \u0026#39;#7663b0\u0026#39;] , x_label = \u0026#39;Day of week\u0026#39; , y_label = \u0026#39;Proportion of check outs\u0026#39; , title = \u0026#39;Check Outs By Registration Status and Day of Week (0 = Sunday)\u0026#39;)    从这幅图你看出了什么？工作日 VS 节假日 为什么会有这样的差别？周末随意  分组柱状图  多级类间绝对数值比较  1  mean_by_reg_co_day.head()   registered\tcasual\ttotal\treg_prop\tcasual_prop weekday\t0\t2890.533333\t1338.295238\t4228.828571\t0.683531\t0.316469 1\t3663.990476\t674.133333\t4338.123810\t0.844603\t0.155397 2\t3954.480769\t556.182692\t4510.663462\t0.876696\t0.123304 3\t3997.394231\t551.144231\t4548.538462\t0.878830\t0.121170 4\t4076.298077\t590.961538\t4667.259615\t0.873381\t0.126619 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # 绘制分组柱状图的函数 def groupedbarplot(x_data, y_data_list, y_data_names, colors, x_label, y_label, title): _, ax = plt.subplots() # 设置每一组柱状图的宽度 total_width = 0.8 # 设置每一个柱状图的宽度 ind_width = total_width / len(y_data_list) # 计算每一个柱状图的中心偏移 alteration = np.arange(-total_width/2+ind_width/2, total_width/2+ind_width/2, ind_width) # 分别绘制每一个柱状图 for i in range(0, len(y_data_list)): # 横向散开绘制 ax.bar(x_data + alteration[i], y_data_list[i], color = colors[i], label = y_data_names[i], width = ind_width) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) ax.legend(loc = \u0026#39;upper right\u0026#39;) # 调用绘图函数 groupedbarplot(x_data = mean_by_reg_co_day.index.values , y_data_list = [mean_by_reg_co_day[\u0026#39;registered\u0026#39;], mean_by_reg_co_day[\u0026#39;casual\u0026#39;]] , y_data_names = [\u0026#39;Registered\u0026#39;, \u0026#39;Casual\u0026#39;] , colors = [\u0026#39;#539caf\u0026#39;, \u0026#39;#7663b0\u0026#39;] , x_label = \u0026#39;Day of week\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Check Outs By Registration Status and Day of Week (0 = Sunday)\u0026#39;)    偏移前：ind_width/2 偏移后：total_width/2 偏移量：total_width/2-ind_width/2  箱式图  多级类间数据分布比较 柱状图 + 堆叠灰度图  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # 只需要指定分类的依据，就能自动绘制箱式图 days = np.unique(daily_data[\u0026#39;weekday\u0026#39;]) bp_data = [] for day in days: bp_data.append(daily_data[daily_data[\u0026#39;weekday\u0026#39;] == day][\u0026#39;cnt\u0026#39;].values) # 定义绘图函数 def boxplot(x_data, y_data, base_color, median_color, x_label, y_label, title): _, ax = plt.subplots() # 设置样式 ax.boxplot(y_data # 箱子是否颜色填充 , patch_artist = True # 中位数线颜色 , medianprops = {\u0026#39;color\u0026#39;: base_color} # 箱子颜色设置，color：边框颜色，facecolor：填充颜色 , boxprops = {\u0026#39;color\u0026#39;: base_color, \u0026#39;facecolor\u0026#39;: median_color} # 猫须颜色whisker , whiskerprops = {\u0026#39;color\u0026#39;: median_color} # 猫须界限颜色whisker cap , capprops = {\u0026#39;color\u0026#39;: base_color}) # 箱图与x_data保持一致 ax.set_xticklabels(x_data) ax.set_ylabel(y_label) ax.set_xlabel(x_label) ax.set_title(title) # 调用绘图函数 boxplot(x_data = days , y_data = bp_data , base_color = \u0026#39;b\u0026#39; , median_color = \u0026#39;r\u0026#39; , x_label = \u0026#39;Day of week\u0026#39; , y_label = \u0026#39;Check outs\u0026#39; , title = \u0026#39;Total Check Outs By Day of Week (0 = Sunday)\u0026#39;)   简单总结  关联分析、数值比较：散点图、曲线图 分布分析：灰度图、密度图 涉及分类的分析：柱状图、箱式图  seaborn单维度与关联维度分析 单变量分布 1 2 3 4 5 6 7 8  %matplotlib inline import numpy as np import pandas as pd from scipy import stats, integrate import matplotlib.pyplot as plt import seaborn as sns sns.set(color_codes=True) np.random.seed(sum(map(ord, \u0026#34;distributions\u0026#34;)))   1 2 3 4 5 6 7 8 9 10 11 12 13  import datetime import matplotlib.dates as mpd import matplotlib.pyplot as plt import matplotlib.ticker as ticker import numpy as np import pandas as pd import time from matplotlib import gridspec from pandas import DataFrame from pandas import Series import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) %matplotlib inline   灰度图 最方便快捷的方式~\n1 2  x = np.random.normal(size=100) sns.distplot(x, kde=True)   想得到更精细的刻画？调节bins~\n1  sns.distplot(x, kde=True, bins=20)   想配合着实例一起看？\n1  sns.distplot(x, kde=False, bins=20, rug=True)   配合着实例一起看有什么好处？指导你设置合适的bins。\n核密度估计 通过观测估计概率密度函数的形状。\n有什么用呢？待定系数法求概率密度函数~\n核密度估计的步骤：\n 每一个观测附近用一个正态分布曲线近似 叠加所有观测的正太分布曲线 归一化\n在seaborn中怎么画呢？  1  sns.kdeplot(x)#体现数据的密集程度   bandwidth的概念：用于近似的正态分布曲线的宽度。\n1 2 3 4  sns.kdeplot(x) sns.kdeplot(x, bw=.2, label=\u0026#34;bw: 0.2\u0026#34;) sns.kdeplot(x, bw=2, label=\u0026#34;bw: 2\u0026#34;) plt.legend()   模型参数拟合 1 2  x = np.random.gamma(6, size=200) sns.distplot(x, kde=False, fit=stats.gamma)   双变量分布 1 2 3  mean, cov = [0, 1], [(1, .5), (.5, 1)] data = np.random.multivariate_normal(mean, cov, 200) df = pd.DataFrame(data, columns=[\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;])   两个相关的正态分布~\n散点图 1  sns.jointplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=df)   六角箱图 1 2 3  x, y = np.random.multivariate_normal(mean, cov, 1000).T with sns.axes_style(\u0026#34;ticks\u0026#34;): sns.jointplot(x=x, y=y, kind=\u0026#34;hex\u0026#34;)   核密度估计 1  sns.jointplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=df, kind=\u0026#34;kde\u0026#34;)   1 2 3 4  f, ax = plt.subplots(figsize=(6, 6)) sns.kdeplot(df.x, df.y, ax=ax) sns.rugplot(df.x, color=\u0026#34;g\u0026#34;, ax=ax) sns.rugplot(df.y, vertical=True, ax=ax)   想看到更连续梦幻的效果~\n1 2 3  f, ax = plt.subplots(figsize=(6, 6)) cmap = sns.cubehelix_palette(as_cmap=True, dark=1, light=0) sns.kdeplot(df.x, df.y, cmap=cmap, n_levels=60, shade=True)   1 2 3 4  g = sns.jointplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=df, kind=\u0026#34;kde\u0026#34;, color=\u0026#34;m\u0026#34;) # g.plot_joint(plt.scatter, c=\u0026#34;w\u0026#34;, s=30, linewidth=1, marker=\u0026#34;+\u0026#34;) # g.ax_joint.collections[0].set_alpha(0) # g.set_axis_labels(\u0026#34;$X$\u0026#34;, \u0026#34;$Y$\u0026#34;)   数据集中的两两关系 1 2  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) iris.head()   \tsepal_length\tsepal_width\tpetal_length\tpetal_width\tspecies 0\t5.1\t3.5\t1.4\t0.2\tsetosa 1\t4.9\t3.0\t1.4\t0.2\tsetosa 2\t4.7\t3.2\t1.3\t0.2\tsetosa 3\t4.6\t3.1\t1.5\t0.2\tsetosa 4\t5.0\t3.6\t1.4\t0.2\tsetosa 1  sns.pairplot(iris);   属性两两间的关系 + 属性的灰度图\n1 2 3  g = sns.PairGrid(iris) g.map_diag(sns.kdeplot) g.map_offdiag(sns.kdeplot, cmap=\u0026#34;Blues_d\u0026#34;, n_levels=20)   seaborn高级分析-连续变量关联分析 1 2 3 4 5 6 7 8 9 10 11 12  %matplotlib inline import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns sns.set(color_codes=True) np.random.seed(sum(map(ord, \u0026#34;regression\u0026#34;))) import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) tips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head()   \ttotal_bill\ttip\tsex\tsmoker\tday\ttime\tsize 0\t16.99\t1.01\tFemale\tNo\tSun\tDinner\t2 1\t10.34\t1.66\tMale\tNo\tSun\tDinner\t3 2\t21.01\t3.50\tMale\tNo\tSun\tDinner\t3 3\t23.68\t3.31\tMale\tNo\tSun\tDinner\t2 4\t24.59\t3.61\tFemale\tNo\tSun\tDinner\t4 1  tips[tips[\u0026#39;size\u0026#39;]==1]   \ttotal_bill\ttip\tsex\tsmoker\tday\ttime\tsize 67\t3.07\t1.00\tFemale\tYes\tSat\tDinner\t1 82\t10.07\t1.83\tFemale\tNo\tThur\tLunch\t1 111\t7.25\t1.00\tFemale\tNo\tSat\tDinner\t1 222\t8.58\t1.92\tMale\tYes\tFri\tLunch\t1 绘制线性回归模型 最简单的方式：散点图 + 线性回归 + 95%置信区间\n1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips)   对于变量离线取值，散点图就显得有些尴尬\n1  sns.lmplot(x=\u0026#34;size\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips)   方法1：加个小的jitter\n1  sns.lmplot(x=\u0026#34;size\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips, x_jitter=.08)   方法2：离散取值上用均值和置信区间代替散点\n1  sns.lmplot(x=\u0026#34;size\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips, x_estimator=np.mean,ci=95)   拟合不同模型 有些时候线性拟合效果不错，有些时候差强人意~\n1 2  anscombe = sns.load_dataset(\u0026#34;anscombe\u0026#34;) sns.lmplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;I\u0026#39;\u0026#34;), ci=None, scatter_kws={\u0026#34;s\u0026#34;: 80})   1  sns.lmplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;II\u0026#39;\u0026#34;), ci=None, scatter_kws={\u0026#34;s\u0026#34;: 80})   高阶拟合\n1 2  #order=2代表x2 sns.lmplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;II\u0026#39;\u0026#34;), order=2, ci=None, scatter_kws={\u0026#34;s\u0026#34;: 80})   处理异常值\n1  sns.lmplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;III\u0026#39;\u0026#34;), robust=True, ci=None, scatter_kws={\u0026#34;s\u0026#34;: 80})   二值变量拟合\n1 2  tips[\u0026#34;big_tip\u0026#34;] = (tips.tip / tips.total_bill) \u0026gt; .15 sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;big_tip\u0026#34;, data=tips, y_jitter=.05)   二值变量用直线模拟不了\n1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;big_tip\u0026#34;, data=tips, logistic=True, y_jitter=.03, ci=None)   如何评价拟合效果？残差曲线~\n1  sns.residplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;I\u0026#39;\u0026#34;), scatter_kws={\u0026#34;s\u0026#34;: 80})   拟合的好，就是白噪声的分布 𝑁(0,𝜎2) 拟合的差，就能看出一些模式\n1  sns.residplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, data=anscombe.query(\u0026#34;dataset == \u0026#39;II\u0026#39;\u0026#34;), scatter_kws={\u0026#34;s\u0026#34;: 80})   变量间的条件关系摸索 1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;day\u0026#34;, data=tips)   1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=tips, markers=[\u0026#34;o\u0026#34;, \u0026#34;x\u0026#34;])   增加更多的分类条件\n1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, col=\u0026#34;time\u0026#34;, data=tips)   1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, col=\u0026#34;time\u0026#34;, row=\u0026#34;sex\u0026#34;, data=tips)   控制图片的大小和形状 1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, col=\u0026#34;day\u0026#34;, data=tips, col_wrap=2, size=5)   1  sns.lmplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, col=\u0026#34;day\u0026#34;, data=tips, aspect=0.5)   "
},
{
	"uri": "https://lczen.github.io/en/tags/numpy/",
	"title": "numpy",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/dataanalysis/",
	"title": "数据分析包",
	"tags": ["numpy"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "%config ZMQInteractiveShell.ast_node_interactivity='all' %pprint numpy ###切片索引\n1 2 3 4  a = np.array([[1,2,3], [4,5,6]]) b= np.random.random((2,2)) # astype做类型转换 a.astype(np.float)   数组索引与切片/Array indexing and slicing\na[:, 1:3] array([[[4, 5, 6]], [[2, 4, 5]]]) a \u0026gt; 2 array([[[False, False, True], [ True, True, True]], [[False, False, True], [False, True, True]]]) a[a\u0026gt;2] #根据条件选择 array([3, 4, 5, 6, 3, 4, 5])#筛选出的是一维的结果 a[a\u0026gt;2] = 0 #根据条件选择数据进行赋值 数学运算 不同维度加法\na a.shape array([[[1, 2, 0], [0, 0, 0]], [[1, 2, 0], [2, 0, 0]]]) c = np.random.random((2,3)) c c.shape array([[0.57104354, 0.82210532, 0.99865874], [0.63489647, 0.51821603, 0.1772419 ]]) (2, 3) a + c array([[[1.57104354, 2.82210532, 0.99865874], [0.63489647, 0.51821603, 0.1772419 ]], [[1.57104354, 2.82210532, 0.99865874], [2.63489647, 0.51821603, 0.1772419 ]]]) 广播特性/boardcasting 1 2 3  a=np.random.random((8,1,6)) b=np.random.random((1,6,1)) (a+b).shape #(8, 6, 6)   统计数学运算(sum,mean) 1 2  a np.sum(a) #全部元素求和   array([[[0.13169241, 0.12076024, 0.94718171, 0.9868782 , 0.52460178,\n0.7050304 ]],\n[[0.75982848, 0.12849312, 0.99862352, 0.57892246, 0.93388848,\n0.27688479]],\n[[0.84861432, 0.46856831, 0.91133697, 0.43440833, 0.73288593,\n0.55042371]],\n[[0.14429995, 0.43673552, 0.39708228, 0.23377411, 0.81263233,\n0.53145192]],\n[[0.42747949, 0.99082152, 0.09460188, 0.02920687, 0.86489352,\n0.25205818]],\n[[0.15014402, 0.66933541, 0.54412228, 0.80764711, 0.09997845,\n0.73227461]],\n[[0.01941847, 0.97843217, 0.28262159, 0.27665419, 0.8822287 ,\n0.37638411]],\n[[0.35989009, 0.39787517, 0.83485582, 0.60017681, 0.86968622,\n0.18526147]]])\n25.321047407612056\n1  np.sum(a, axis=0) #按照第1个维度汇总求和   array([[2.84136723, 4.19102145, 5.01042605, 3.94766809, 5.72079541,\n3.60976919]])\n1  np.sum(a, axis=2) #按照第3个维度汇总求和   sum函数axis = 0的时候：\nsum函数axis = 1的时候：\nsum函数axis = 2的时候：\n乘法dot,multiply np.dot是点乘(矩阵乘法) |A B| . |E F| = |A*E+B*G A*F+B*H| |C D| |G H| |C*E+D*G C*F+D*H| np.multiply是逐元素乘法 |A B| ⊙ |E F| = |A*E B*F| |C D| |G H| |C*G D*H| 行给值操作 1 2 3 4 5  a=np.random.random((8,2)) a predict_mean_ci_low, predict_mean_ci_upp = a.T predict_mean_ci_low predict_mean_ci_upp   array([[0.59479112, 0.11532206], [0.66448315, 0.22538265], [0.1639891 , 0.79296798], [0.15639683, 0.18279829], [0.26906626, 0.94362871], [0.39945426, 0.06276308], [0.47098298, 0.71271548], [0.8769299 , 0.35743786]]) array([0.59479112, 0.66448315, 0.1639891 , 0.15639683, 0.26906626, 0.39945426, 0.47098298, 0.8769299 ]) array([0.11532206, 0.22538265, 0.79296798, 0.18279829, 0.94362871, 0.06276308, 0.71271548, 0.35743786]) Pandas数据结构Series 构造和初始化Series import pandas as pd **pd.__version__ #查看版本，不同版本的pandas功能有一些差异** s = pd.Series([7, 'Beijing', 2.17, -12344, 'Happy Birthday!']) s 0 7\n1 Beijing\n2 2.17\n3 -12344\n4 Happy Birthday!\ndtype: object\n1 2 3 4 5  s = pd.Series([7, \u0026#39;Beijing\u0026#39;, 2.17, -12344, \u0026#39;Happy Birthday!\u0026#39;]) #指定index s = pd.Series([7, \u0026#39;Beijing\u0026#39;, 2.17, -12344, \u0026#39;Happy Birthday!\u0026#39;], index=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;]) s   A 7\nB Beijing\nC 2.17\nD -12344\nE Happy Birthday!\ndtype: object\ndictionary创建Series 还可以用dictionary来构造一个Series，因为Series本来就是key value pairs。\n1 2 3  cities = {\u0026#39;Beijing\u0026#39;: 55000, \u0026#39;Shanghai\u0026#39;: 60000, \u0026#39;Shenzhen\u0026#39;: 50000, \u0026#39;Hangzhou\u0026#39;: 20000, \u0026#39;Guangzhou\u0026#39;: 25000, \u0026#39;Suzhou\u0026#39;: None} apts = pd.Series(cities, name=\u0026#34;price\u0026#34;) apts   Beijing 55000.0\nShanghai 60000.0\nShenzhen 50000.0\nHangzhou 20000.0\nGuangzhou 25000.0\nSuzhou NaN\nName: price, dtype: float64\nnumpy ndarray构建一个Series 1  s = pd.Series(np.random.randn(5), index=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;])   Series选择数据 1 2 3 4 5 6 7 8 9 10 11 12 13  apts[1:] #切片 type(apts[:-1])#切片\u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; **apts[[4,3,1]]** #这种写法是Series独有而list不具备的 apts[1:] + apts[:-1] #注意，在相加的时候，其实是index对齐的方式相加的,其中一个没有结果是NaN apts[\u0026#34;Hangzhou\u0026#34;]#不显示index，只显示value apts[[\u0026#34;Hangzhou\u0026#34;, \u0026#34;Beijing\u0026#34;, \u0026#34;Shenzhen\u0026#34;]]#显示index和value \u0026#34;Hangzhou\u0026#34; **in** apts #判断是否有这个index apts.get(\u0026#34;Chongqing\u0026#34;, 55) #和字典类似，默认值放在后面 apts[apts \u0026lt; 50000] #条件选择 apts.median() #中位数，统计计算 less_than_50000 = apts \u0026lt; 50000 less_than_50000 #实际上拿到的是一个True or False的series print(apts[less_than_50000])   Series元素赋值 1 2  apts[\u0026#39;Shenzhen\u0026#39;] = 55000 apts[apts \u0026lt;= 50000] = 40000#条件赋值   Series数学运算 1 2 3 4  apts / 2 #广播特性 apts ** 2 #广播特性 np.square(apts) print(cars + apts * 100) #注意广播特性与index对齐求和   数据结构Dataframe 创建一个DataFrame dataframe可以由一个dictionary构造得到。\n1 2 3 4 5  import pandas as pd data = {\u0026#39;city\u0026#39;: [\u0026#39;Beijing\u0026#39;, \u0026#39;Shanghai\u0026#39;, \u0026#39;Guangzhou\u0026#39;, \u0026#39;Shenzhen\u0026#39;, \u0026#39;Hangzhou\u0026#39;, \u0026#39;Chongqing\u0026#39;], \u0026#39;year\u0026#39;: [2016,2017,2016,2017,2016, 2016], \u0026#39;population\u0026#39;: [2100, 2300, 1000, 700, 500, 500]} pd.DataFrame(data)   \tcity\tyear\tpopulation 0\tBeijing\t2016\t2100 1\tShanghai\t2017\t2300 2\tGuangzhou\t2016\t1000 3\tShenzhen\t2017\t700 4\tHangzhou\t2016\t500 5\tChongqing\t2016\t500 columns的名字和顺序可以指定\n1  pd.DataFrame(data, columns=[\u0026#39;year\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;population\u0026#39;])   index也可以指定\n1 2  frame = pd.DataFrame(data, columns = [\u0026#39;year\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;population\u0026#39;], index = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;, \u0026#39;six\u0026#39;])   也可以从几个Series构建一个DataFrame\n1 2  # index会对齐 df = pd.DataFrame({\u0026#34;apts\u0026#34;: apts, \u0026#34;cars\u0026#34;: cars})   也可以用一个list of dicts来构建DataFrame\n1 2  data = [{\u0026#34;BOSS\u0026#34;: 999999, \u0026#34;Jason\u0026#34;: 50000, \u0026#34;Han\u0026#34;: 1000}, {\u0026#34;BOSS\u0026#34;: 99999, \u0026#34;Jason\u0026#34;: 8000, \u0026#34;Han\u0026#34;: 200}] pd.DataFrame(data,columns=[\u0026#39;BOSS\u0026#39;,\u0026#39;Han\u0026#39;,\u0026#39;Jason\u0026#39;])   \tBOSS\tHan\tJason 0\t999999\t1000\t50000 1\t99999\t200\t8000 1  pd.DataFrame(data, index=[\u0026#34;salary\u0026#34;, \u0026#34;bonus\u0026#34;])   \tBOSS\tHan\tJason salary\t999999\t1000\t50000 bonus\t99999\t200\t8000 根据column取数据\n1  df[\u0026#34;apts\u0026#34;]   算数运算新建一个column\n1  df[\u0026#34;total_cost\u0026#34;] = df[\u0026#34;apts\u0026#34;]*100 + df[\u0026#34;cars\u0026#34;]   apts\tcars\ttotal_cost Beijing\t55000.0\t300000.0\t5800000.0 Chongqing\tNaN\t150000.0\tNaN Guangzhou\t40000.0\t200000.0\t4200000.0 Hangzhou\t40000.0\tNaN\tNaN Shanghai\t60000.0\t400000.0\t6400000.0 Shenzhen\t55000.0\t300000.0\t5800000.0 Suzhou\tNaN\tNaN\tNaN Tianjin\tNaN\t200000.0\tNaN 1 2 3 4 5  # 注意一下Series与DataFrame的区别 frame[\u0026#39;city\u0026#39;] #Series type(frame[\u0026#39;city\u0026#39;]) frame[[\u0026#39;city\u0026#39;]] #DataFrame type(frame[[\u0026#39;city\u0026#39;]])   loc方法可以拿到行\n1 2 3  # loc+index取出行 frame.loc[\u0026#39;three\u0026#39;] type(frame.loc[\u0026#39;three\u0026#39;])#Series   iloc方法可以拿到行和列，把pandas dataframe当做numpy的ndarray来操作\n1  frame.head()   \tyear\tcity\tpopulation one\t2016\tBeijing\t2100 two\t2017\tShanghai\t2300 three\t2016\tGuangzhou\t1000 four\t2017\tShenzhen\t700 five\t2016\tHangzhou\t500 1  frame.iloc[1:3, 2:4] #类似于numpy的切片   \tpopulation two\t2300 three\t1000 DataFrame数据选择与赋值 大多数情况下，我们的赋值操作都是在数据选择之后进行赋值的，DataFrame的数据选择有很多不同的函数，相对比较自由。但是建议大家使用loc函数进行数据选择。\n具体的语法为df.loc[行条件,列条件]\n1 2 3  frame.loc[\u0026#34;one\u0026#34;,\u0026#34;population\u0026#34;] = 2200 frame.loc[:,\u0026#39;debt\u0026#39;] = 100#给一整列赋值 frame.loc[\u0026#39;six\u0026#39;,:] = 0#给一整行赋值   我们可以给定多个条件进行组合，选择对应的数据。每个条件用小括号包裹，同时通过\u0026amp;(与) |(或) ~(非)进行组合。\n1 2  frame.loc[(frame[\u0026#39;year\u0026#39;]==2016)\u0026amp;(frame[\u0026#39;city\u0026#39;]!=\u0026#39;Shanghai\u0026#39;)\u0026amp;(frame[\u0026#39;population\u0026#39;]\u0026gt;=1000),:] frame.loc[(frame[\u0026#39;year\u0026#39;]==2016)|(frame[\u0026#39;population\u0026#39;]\u0026gt;=1000),[\u0026#39;year\u0026#39;,\u0026#39;city\u0026#39;,\u0026#39;population\u0026#39;]]   \tyear\tcity\tpopulation\tdebt one\t2016\tBeijing\t2200\t100 three\t2016\tGuangzhou\t1000\t100 year\tcity\tpopulation one\t2016\tBeijing\t2200 two\t2017\tShanghai\t2300 three\t2016\tGuangzhou\t1000 five\t2016\tHangzhou\t500 还可以用Series来指定需要修改的index以及相对应的value，没有指定的默认用NaN.\n1 2  val = pd.Series([100, 200, 300], index=[\u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;five\u0026#39;]) frame[\u0026#39;debt\u0026#39;] = val    year city population debt one 2016 Beijing 2200 NaN two 2017 Shanghai 2300 100.0 three 2016 Guangzhou 1000 200.0 four 2017 Shenzhen 700 NaN five 2016 Hangzhou 500 300.0 six 0 0 0 NaN 如果我们想要知道有哪些列，直接用columns\n1 2  print(frame.columns) frame.index   一个DataFrame就和一个numpy 2d array一样，可以被转置\n1 2 3  pop = {\u0026#39;Beijing\u0026#39;: {2016: 2100, 2017:2200}, \u0026#39;Shanghai\u0026#39;: {2015:2400, 2016:2500, 2017:2600}} frame.T   \tone\ttwo\tthree\tfour\tfive\tsix year\t2016\t2017\t2016\t2017\t2016\t0 city\tBeijing\tShanghai\tGuangzhou\tShenzhen\tHangzhou\t0 population\t2200\t2300\t1000\t700\t500\t0 debt\tNaN\t100\t200\tNaN\t300\tNaN csv文件读写  read_csv to_csv  1 2 3 4 5 6  import pandas as pd pokemon = pd.read_csv(\u0026#34;../data/Pokemon.csv\u0026#34;) pokemon.head(10) #describe可以帮我们了解数据的统计特性 pokemon.describe() pokemon.loc[:50,[\u0026#39;HP\u0026#39;,\u0026#39;Defense\u0026#39;]].to_csv(\u0026#39;../data/pokemon_tmp.csv\u0026#39;)   数据缺失 reference\n1 2 3 4  #选择Type 2为空的那行数据，把Type 2列的数据设为Type 1 pokemon.loc[pokemon[\u0026#34;Type 2\u0026#34;].isnull(), \u0026#34;Type 2\u0026#34;] = pokemon.loc[pokemon[\u0026#34;Type 2\u0026#34;].isnull(), \u0026#34;Type 1\u0026#34;] print(apts[apts.isnull()]) print(apts[apts.isnull() == False]) #根据是否缺失选择数据   用to_csv方法写出到文件\n1 2 3  import numpy as np df = pd.DataFrame(np.random.rand(10, 4), columns=list(\u0026#34;abcd\u0026#34;)) df.to_csv(\u0026#34;../data/sample.tsv\u0026#34;, sep=\u0026#34;\\t\u0026#34;)   a\tb\tc\td 0\t0.569415\t0.468089\t0.260840\t0.172120 1\t0.288680\t0.771640\t0.465192\t0.979539 2\t0.095298\t0.385866\t0.983792\t0.412143 3\t0.903898\t0.327998\t0.134250\t0.948132 4\t0.955035\t0.838292\t0.932714\t0.630797 5\t0.210932\t0.428113\t0.937603\t0.650599 6\t0.661533\t0.390347\t0.080175\t0.159178 7\t0.018230\t0.961200\t0.043113\t0.877882 8\t0.381884\t0.314338\t0.273786\t0.200851 9\t0.454861\t0.283102\t0.969393\t0.404643 1  df.add(pd.Series(np.ones(10)) * 10, axis=0)#加法   数据删除 DataFrame中的数据删除，可以用drop完成，需要特别注意的是，DataFrame的很多操作(包括drop)都有一个inplace参数，默认inplace=False，我们会对数据进行变换(比如删除行列)然后以返回值形态返回结果，如果inplace=True则会就地变换。\n1 2  frame.drop([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;])#删除列表中的行 frame.drop(\u0026#39;debt\u0026#39;, axis=1)#删除列   pandas下 分组与聚合 分组/groupby 1 2 3 4 5 6 7 8 9 10  import pandas as pd import numpy as np %matplotlib inline salaries = pd.DataFrame({ \u0026#39;Name\u0026#39;: [\u0026#39;BOSS\u0026#39;, \u0026#39;Jason\u0026#39;, \u0026#39;Jason\u0026#39;, \u0026#39;Han\u0026#39;, \u0026#39;BOSS\u0026#39;, \u0026#39;BOSS\u0026#39;, \u0026#39;Jason\u0026#39;, \u0026#39;BOSS\u0026#39;], \u0026#39;Year\u0026#39;: [2016,2016,2016,2016,2017,2017,2017,2017], \u0026#39;Salary\u0026#39;: [10000,2000,4000,5000,18000,25000,3000,4000], \u0026#39;Bonus\u0026#39;: [3000,1000,1000,1200,4000,2300,500,1000] }) salaries   \tName\tYear\tSalary\tBonus 0\tBOSS\t2016\t10000\t3000 1\tJason\t2016\t2000\t1000 2\tJason\t2016\t4000\t1000 3\tHan\t2016\t5000\t1200 4\tBOSS\t2017\t18000\t4000 5\tBOSS\t2017\t25000\t2300 6\tJason\t2017\t3000\t500 7\tBOSS\t2017\t4000\t1000 group_by_name = salaries.groupby('Name') 聚合/aggregates 1  group_by_name.sum()#按照名字取sum   默认会排序，也可以选择不排序\n1 2 3  #两形式相同 salaries.groupby(\u0026#39;Name\u0026#39;, sort=False).sum() group_by_name.aggregate(sum)   group by的attributes\n1 2  print(group_by_name.groups) print(len(group_by_name))   {'BOSS': Int64Index([0, 4, 5, 7], dtype='int64'), 'Han': Int64Index([3], dtype='int64'), 'Jason': Int64Index([1, 2, 6], dtype='int64')} 3 可以用多个columns做group by\n1 2  group_by_name_year = salaries.groupby([\u0026#39;Name\u0026#39;, \u0026#39;Year\u0026#39;]) group_by_name_year.sum()   size,mean,median\n1 2 3 4  group_by_name_year.size() group_by_name.mean() group_by_name.median() group_by_name.describe()   遍历分组 1 2 3 4 5  for name, group in group_by_name: print(name) print(\u0026#34;=====\u0026#34;) print(group) print(\u0026#39;\\n\u0026#39;)   BOSS ===== Name Year Salary Bonus 0 BOSS 2016 10000 3000 4 BOSS 2017 18000 4000 5 BOSS 2017 25000 2300 7 BOSS 2017 4000 1000 Han ===== Name Year Salary Bonus 3 Han 2016 5000 1200 Jason ===== Name Year Salary Bonus 1 Jason 2016 2000 1000 2 Jason 2016 4000 1000 6 Jason 2017 3000 500 选择一个group\n1  group_by_name.get_group(\u0026#34;Jason\u0026#34;)   Name\tYear\tSalary\tBonus 1\tJason\t2016\t2000\t1000 2\tJason\t2016\t4000\t1000 6\tJason\t2017\t3000\t500 agg是aggregate的alias，可以替代使用\n1  group_by_name.agg([np.sum, np.mean, np.std])   Year\tSalary\tBonus sum\tmean\tstd\tsum\tmean\tstd\tsum\tmean\tstd Name\tBOSS\t8067\t2016.750000\t0.50000\t57000\t14250\t9178.779875\t10300\t2575.000000\t1260.621540 Han\t2016\t2016.000000\tNaN\t5000\t5000\tNaN\t1200\t1200.000000\tNaN Jason\t6049\t2016.333333\t0.57735\t9000\t3000\t1000.000000\t2500\t833.333333\t288.675135 对每一列可以采用不同的aggregate操作\n1  group_by_name.agg({\u0026#34;Bonus\u0026#34;: np.sum, \u0026#34;Salary\u0026#34;: np.sum, \u0026#34;Year\u0026#34;: (lambda x: list(x)[0])})   \tBonus\tSalary\tYear Name\tBOSS\t10300\t57000\t2016 Han\t1200\t5000\t2016 Jason\t2500\t9000\t2016 1  group_by_name.describe()   transform vs apply transform会把group中的每一个record都按照同样的规则转化\n1 2 3  nvda = pd.read_csv(\u0026#34;../data/NVDA.csv\u0026#34;, index_col=0, parse_dates=[0]) nvda.index nvda.groupby(nvda.index.year).mean()   transform\n1 2 3  zscore = lambda x:(x-x.mean())/x.std() transformed = nvda.groupby(nvda.index.year).transform(zscore) transformed.head()   用另一个function apply 也可以起到同样的效果\n1 2 3 4 5 6  def my_trans(x): return x**2 if x \u0026gt; 1.68 else x nvda.loc[:,\u0026#39;new\u0026#39;] = nvda[\u0026#39;Close\u0026#39;].apply(my_trans) nvda.head() transformed = nvda.groupby(nvda.index.year).apply(zscore) transformed.head()   我们来尝试一下把这些数据画出来\n1 2 3  compare = pd.DataFrame({\u0026#34;Original Adj Close\u0026#34;: nvda[\u0026#34;Adj Close\u0026#34;], \u0026#34;Transormed Adj Close\u0026#34;: transformed[\u0026#34;Adj Close\u0026#34;]}) compare.plot()   给定范围\n1 2 3  price_range = lambda x: x.max() - x.min() nvda.groupby(nvda.index.year).transform(price_range).head() nvda.groupby(nvda.index.year).transform(\u0026#34;max\u0026#34;).head()   filter 比如我们想要找出符合统计特征的一些内容，就像SQL里面的having, where语句\n1 2 3 4 5 6 7 8 9 10  s = pd.Series([1,1,2,2,2,3,4,4,5]) s s.groupby(s).groups s.groupby(s).filter(lambda x: x.sum() \u0026gt; 4)#每个类别总和大于4 df = pd.DataFrame({\u0026#34;A\u0026#34;: np.arange(8), \u0026#34;B\u0026#34;:list(\u0026#34;aaabbbcc\u0026#34;)}) df df.groupby(\u0026#34;B\u0026#34;).filter(lambda x: len(x) \u0026gt; 2)#分组以后，组中元素至少两个.这里x是针对组 nvda.groupby([nvda.index.year, nvda.index.month]).filter(lambda x: x[\u0026#34;Adj Close\u0026#34;].mean() \u0026gt; 50).head() nvda.groupby([nvda.index.year, nvda.index.month,nvda.index.day]).filter(lambda x: x[\u0026#34;Adj Close\u0026#34;]\u0026gt;100).head()   小结 Group by: split-apply-combine\n 首先第一步是分离数据split，按照一定的规则把数据分成几类。 第二步是对每一部分数据都做一定的操作，这个操作可以是汇总操作aggregate，可以是一个变换transform，也可以是过滤数据filter。 最后一步就是把处理过的数据再合成一张DataFrame。  多表合并与拼接  concat append merge join  1 2 3 4 5 6 7 8 9 10 11  df1 = pd.DataFrame({\u0026#39;apts\u0026#39;: [55000, 60000], \u0026#39;cars\u0026#39;: [200000, 300000],}, index = [\u0026#39;Shanghai\u0026#39;, \u0026#39;Beijing\u0026#39;] ) df2 = pd.DataFrame({\u0026#39;cars\u0026#39;: [150000, 120000], \u0026#39;apts\u0026#39;: [25000, 20000], }, index = [\u0026#39;Hangzhou\u0026#39;, \u0026#39;Nanjing\u0026#39;]) df3 = pd.DataFrame({\u0026#39;apts\u0026#39;: [30000, 10000], \u0026#39;cars\u0026#39;: [180000, 100000],\u0026#39;mar\u0026#39;: [180000, 100000]}, index = [\u0026#39;Guangzhou\u0026#39;, \u0026#39;Chongqing\u0026#39;])   concat 1 2 3 4  import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) frames = [df1, df2, df3] result = pd.concat([df1,df2,df3])   在concat的时候可以指定keys，这样可以给每一个部分加上一个Key。\n以下的例子就构造了一个hierarchical index。\n1 2  result2 = pd.concat(frames, keys=[\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;], sort=False) print(result2)    apts cars mar x Shanghai 55000 200000 NaN Beijing 60000 300000 NaN y Hangzhou 25000 150000 NaN Najing 20000 120000 NaN z Guangzhou 30000 180000 180000.0 Chongqing 10000 100000 100000.0 选择数据\n1  result2.loc[\u0026#34;y\u0026#34;,\u0026#34;apts\u0026#34;]   1 2 3  df4 = pd.DataFrame({\u0026#39;salaries\u0026#39;: [10000, 30000, 30000, 20000, 15000]}, index = [\u0026#39;Suzhou\u0026#39;, \u0026#39;Beijing\u0026#39;, \u0026#39;Shanghai\u0026#39;, \u0026#39;Guangzhou\u0026#39;, \u0026#39;Tianjin\u0026#39;]) result3 = pd.concat([result, df4], axis=1, sort=False)   用inner可以去掉NaN,也就是说如果出现了不匹配的行就会被忽略\n1  result3 = pd.concat([result, df4], axis=1, join=\u0026#39;inner\u0026#39;)   用append来做concat\n1 2  print(df1.append(df2, sort=False)) df1.join   Series和DataFrame还可以被一起concatenate，这时候Series会先被转成DataFrame然后做Join，因为Series本来就是一个只有一维的DataFrame对吧。\n1 2 3 4  s1 = pd.Series([60, 50], index=[\u0026#39;Shanghai\u0026#39;, \u0026#39;Beijing\u0026#39;], name=\u0026#39;meal\u0026#39;) print(s1) print(df1) print(pd.concat([df1, s1], axis=1))   Shanghai 60 Beijing 50 Name: meal, dtype: int64 apts cars Shanghai 55000 200000 Beijing 60000 300000 apts cars meal Shanghai 55000 200000 60 Beijing 60000 300000 50 如何append一个row到DataFrame里。\n1 2 3  s2 = pd.Series([18000, 12000], index=[\u0026#39;apts\u0026#39;, \u0026#39;cars\u0026#39;], name=\u0026#39;Xiamen\u0026#39;) #注意这里的name是必须要有的，因为要用作Index。 print(s2) print(df1.append(s2))   apts 18000 cars 12000 Name: Xiamen, dtype: int64 apts cars Shanghai 55000 200000 Beijing 60000 300000 Xiamen 18000 12000 Merge(Join) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df1 = pd.DataFrame({\u0026#39;apts\u0026#39;: [55000, 60000, 58000], \u0026#39;cars\u0026#39;: [200000, 300000,250000], \u0026#39;city\u0026#39;: [\u0026#39;Shanghai\u0026#39;, \u0026#39;Beijing\u0026#39;,\u0026#39;Shenzhen\u0026#39;]}) df4 = pd.DataFrame({\u0026#39;salaries\u0026#39;: [10000, 30000, 30000, 20000, 15000], \u0026#39;city\u0026#39;: [\u0026#39;Suzhou\u0026#39;, \u0026#39;Beijing\u0026#39;, \u0026#39;Shanghai\u0026#39;, \u0026#39;Guangzhou\u0026#39;, \u0026#39;Tianjin\u0026#39;]}) df1 df4 result = pd.merge(df1, df4, on=\u0026#39;city\u0026#39;) result result = pd.merge(df1, df4) result result = pd.merge(df1, df4, on=\u0026#39;city\u0026#39;, how=\u0026#39;outer\u0026#39;) result result = pd.merge(df1, df4, on=\u0026#39;city\u0026#39;, how=\u0026#39;right\u0026#39;) result result = pd.merge(df1, df4, on=\u0026#39;city\u0026#39;, how=\u0026#39;left\u0026#39;) result   如果要用concat做同样的事情\n1  pd.concat([df1.set_index(\u0026#34;city\u0026#34;), df4.set_index(\u0026#39;city\u0026#39;)], sort=False, axis=1, join=\u0026#34;inner\u0026#34;)   lianjia projects 载入数据 1 2 3  import pandas as pd lj_data = pd.read_csv(\u0026#39;../data/LJdata.csv\u0026#39;) lj_data.columns   大家规范一点，用英文的column name，这样免去了后续的一些问题(主要是编码问题)\n1  lj_data.columns = [\u0026#39;district\u0026#39;, \u0026#39;address\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;house_type\u0026#39;, \u0026#39;area\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;floor\u0026#39;, \u0026#39;build_time\u0026#39;, \u0026#39;direction\u0026#39;, \u0026#39;update_time\u0026#39;, \u0026#39;view_num\u0026#39;, \u0026#39;extra_info\u0026#39;, \u0026#39;link\u0026#39;]   查看数据的形状和信息 info/describe/shape\n1 2 3  lj_data.shape lj_data.info()#查看是否有空数据，isnull lj_data.describe(include=\u0026#39;all\u0026#39;)#all   找到最近更新信息的20套房子 1  lj_data.sort_values(by=\u0026#39;update_time\u0026#39;).tail(20)   平均看房人数 1 2  lj_data[\u0026#39;view_num\u0026#39;].mean() lj_data[\u0026#39;view_num\u0026#39;].median()#中位数   房龄最小的20套房子的平均看房人数、平均面积 1 2 3 4 5 6 7 8 9  import numpy as np def get_age(x): try: return 2019-int(x[:4]) except: return np.NaN lj_data.loc[:,\u0026#39;age\u0026#39;] = lj_data[\u0026#39;build_time\u0026#39;].apply(get_age) lj_data.loc[:,\u0026#39;house_area\u0026#39;] = lj_data[\u0026#39;area\u0026#39;].apply(lambda x:int(x[:-2])) lj_data.nsmallest(columns=\u0026#39;age\u0026#39;, n=20)[[\u0026#39;view_num\u0026#39;, \u0026#39;house_area\u0026#39;]].mean()   房子价格的分布(平均，方差，中位数) 1  lj_data[\u0026#39;price\u0026#39;].describe()   最受欢迎的朝向(平均看房人数) 1 2  tmp = lj_data.loc[:,[\u0026#39;direction\u0026#39;,\u0026#39;view_num\u0026#39;]].groupby(\u0026#39;direction\u0026#39;).agg(\u0026#39;mean\u0026#39;) tmp.reset_index().sort_values(by=\u0026#39;view_num\u0026#39;, ascending=False).head(1)   房型数量分布 1 2 3  lj_data[\u0026#39;house_type\u0026#39;].value_counts() %matplotlib inline lj_data[\u0026#39;house_type\u0026#39;].value_counts().plot(kind=\u0026#39;bar\u0026#39;)   最受欢迎的房型 1 2  tmp = lj_data.loc[:,[\u0026#39;house_type\u0026#39;,\u0026#39;view_num\u0026#39;]].groupby(\u0026#39;house_type\u0026#39;).agg(\u0026#39;mean\u0026#39;) tmp.reset_index().sort_values(by=\u0026#39;view_num\u0026#39;, ascending=False).head(1)   房子的平均租房价格(按平米算) 1 2  lj_data.loc[:,\u0026#39;price_per_m2\u0026#39;] = lj_data[\u0026#39;price\u0026#39;]/lj_data[\u0026#39;house_area\u0026#39;] lj_data.price_per_m2.mean()   最受关注的小区 1 2  tmp = lj_data.loc[:,[\u0026#39;address\u0026#39;,\u0026#39;view_num\u0026#39;]].groupby(\u0026#39;address\u0026#39;).agg(\u0026#39;sum\u0026#39;) tmp.reset_index().sort_values(by=\u0026#39;view_num\u0026#39;, ascending=False).head(1)   出租房源最多的小区 1  lj_data[\u0026#39;address\u0026#39;].value_counts().head(1)   集中供暖和非集中供暖的有多少家，平均价格是多少 1 2  lj_data.loc[:,\u0026#39;center_heating\u0026#39;] = lj_data[\u0026#39;extra_info\u0026#39;].apply(lambda x:\u0026#39;集中供暖\u0026#39; in x) lj_data.loc[:,[\u0026#39;price\u0026#39;,\u0026#39;center_heating\u0026#39;]].groupby(\u0026#39;center_heating\u0026#39;).agg(\u0026#39;mean\u0026#39;)   不同房型的平均/最大/最小面积 1  lj_data.loc[:,[\u0026#39;price\u0026#39;,\u0026#39;house_type\u0026#39;]].groupby(\u0026#39;house_type\u0026#39;).agg([\u0026#39;mean\u0026#39;,\u0026#39;max\u0026#39;,\u0026#39;min\u0026#39;])   哪个地铁口附近的房子最多 1 2 3 4 5 6 7 8 9  import re def get_subway_info(string, pattern, n): result = re.search(pattern, string) if result: return result.group(n) return \u0026#39;\u0026#39; pattern = \u0026#39;距离(.+线(\\(.*?段\\))?)(.+站)\u0026#39; lj_data.loc[:,\u0026#39;subway_station\u0026#39;] = lj_data[\u0026#39;extra_info\u0026#39;].apply(lambda x:get_subway_info(x, pattern, 3)) lj_data[\u0026#39;subway_station\u0026#39;].value_counts()   地铁附近的房子平均价格 比 非地铁的高多少 1 2  lj_data.loc[:,\u0026#39;near_subway\u0026#39;] = lj_data[\u0026#39;subway_station\u0026#39;].apply(lambda x:len(x)\u0026gt;2) lj_data[[\u0026#39;price\u0026#39;,\u0026#39;near_subway\u0026#39;]].groupby(\u0026#39;near_subway\u0026#39;).agg(\u0026#39;mean\u0026#39;)   地铁附近的房源离地铁平均距离 1 2 3  pattern = \u0026#39;(\\d+)米\u0026#39; lj_data.loc[:,\u0026#39;subway_distance\u0026#39;] = lj_data[\u0026#39;extra_info\u0026#39;].apply(lambda x: get_subway_info(x, pattern, 1)) lj_data.loc[lj_data.near_subway==True,\u0026#39;subway_distance\u0026#39;].astype(int).mean()   最多的在租楼层 1  lj_data[\u0026#39;floor\u0026#39;].apply(lambda x:x[:3]).value_counts()   直接看房的房子比例 1 2  lj_data.loc[:,\u0026#39;convenient\u0026#39;] = lj_data[\u0026#39;extra_info\u0026#39;].apply(lambda x:\u0026#39;随时看房\u0026#39; in x) lj_data[\u0026#39;convenient\u0026#39;].value_counts(normalize=True)   "
},
{
	"uri": "https://lczen.github.io/en/tags/python%E7%BB%83%E4%B9%A0%E9%A2%98/",
	"title": "python练习题",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/pythonpractice2/",
	"title": "python练习题2",
	"tags": ["python练习题"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "1 2 3 4 5 6  def test(got, expected):#测试函数 if got == expected: prefix = \u0026#39;正确!\u0026#39; else: prefix = \u0026#39;错误!\u0026#39; print(\u0026#39;%s你的结果: %s应该返回的结果: %s\u0026#39; % (prefix, repr(got), repr(expected)))   数据结构题 习题3 输入一个非空的元组列表，返回按列表中元组的最后一个元素从小到大排序后的元组列表\n例如：输入：[(1, 7), (1, 3), (3, 4, 5), (2, 2)]应该返回：[(2, 2), (1, 3), (3, 4, 5), (1, 7)]\n提示：使用自定义键=函数从每个元组提取最后一个元素。\n1 2 3 4 5 6 7 8 9 10  def last(a): return a[-1] def sort_last(tuples): # +++your code here+++ # LAB(begin solution) return sorted(tuples, key=last) return sorted(tuples, key=lambda d: d[-1]) # 可运行代码自测 test(sort_last([(1, 3), (3, 2), (2, 1)]),[(2, 1), (3, 2), (1, 3)])   习题4 输入一个数字列表，将所有相邻且相同的元素去重保留一个元素后返回\n例如：输入[1, 2, 2, 3] 返回 [1, 2, 3]；输入[1, 1, 2, 2, 3, 3, 3] 返回 [1, 2, 3]\n新建一个列表或者修改原来的列表返回均可。\n1 2 3 4 5 6 7 8 9 10 11 12  def remove_adjacent(nums): # +++your code here+++ # LAB(begin solution) result = [] for num in nums: if len(result) == 0 or num != result[-1]: result.append(num) return result # 可运行代码自测 test(remove_adjacent([1, 2, 2, 3]), [1, 2, 3]) test(remove_adjacent([2, 2, 3, 3, 3]), [2, 3]) test(remove_adjacent([]), [])   习题5 习题5 给定两个按递增顺序排序的列表,创建并返回一个合并的按排序排列的所有元素的列表。\n例如输入 [\u0026lsquo;aa\u0026rsquo;, \u0026lsquo;xx\u0026rsquo;, \u0026lsquo;zz\u0026rsquo;], [\u0026lsquo;bb\u0026rsquo;, \u0026lsquo;cc\u0026rsquo;]，应该返回[\u0026lsquo;aa\u0026rsquo;, \u0026lsquo;bb\u0026rsquo;, \u0026lsquo;cc\u0026rsquo;, \u0026lsquo;xx\u0026rsquo;, \u0026lsquo;zz\u0026rsquo;]\n希望你提供的解决方案在“线性”时间内工作，使两个列表都可以一次完成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def linear_merge(list1, list2): # +++your code here+++ # LAB(begin solution) result = [] # Look at the two lists so long as both are non-empty. # Take whichever element [0] is smaller. while len(list1) and len(list2): if list1[0] \u0026lt; list2[0]: result.append(list1.pop(0)) else: result.append(list2.pop(0)) # Now tack on what\u0026#39;s left result.extend(list1) result.extend(list2) return result   习题5 考虑把一个字符串拆分成两个等分\n 1.如果字符串长度是偶数，前一半和后一半的长度是相同的 2.如果字符串长度是奇数，则多出的一个字符加到前一半，如：\u0026lsquo;abcde\u0026rsquo;，前一半是'abc\u0026rsquo;，后一半是'de\u0026rsquo; 3.输入两个字符串, a 和 b,按以下格式返回结果  a-front + b-front + a-back + b-back    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def front_back(a, b): # +++your code here+++ # LAB(begin solution) # Figure out the middle position of each string. a_middle = len(a) // 2 b_middle = len(b) // 2 if len(a) % 2 == 1: # add 1 if length is odd a_middle = a_middle + 1 if len(b) % 2 == 1: b_middle = b_middle + 1 return a[:a_middle] + b[:b_middle] + a[a_middle:] + b[b_middle:] # 可运行代码自测 test(front_back(\u0026#39;abcd\u0026#39;, \u0026#39;xy\u0026#39;), \u0026#39;abxcdy\u0026#39;) test(front_back(\u0026#39;abcde\u0026#39;, \u0026#39;xyz\u0026#39;), \u0026#39;abcxydez\u0026#39;) test(front_back(\u0026#39;Kitten\u0026#39;, \u0026#39;Donut\u0026#39;), \u0026#39;KitDontenut\u0026#39;)   综合练习题 编写程序完成以下任务：\n读取指定的文件，并使用split()函数以空白为字符串分隔得到文件中所有的单词。\n完成一个名为mimic_dict的函数：\n 以出现在文件中的单词(全都小写化)为键(key)，文件中所有紧跟在单词后面的一个单词组成的列表为值(value)。单词列表可以是任意顺序的，也可以包含重复的值。\n例如，键“and”可能有列表[\u0026ldquo;then\u0026rdquo;, \u0026ldquo;best\u0026rdquo;, \u0026ldquo;then\u0026rdquo;, \u0026ldquo;after\u0026rdquo;, \u0026hellip;]，此列表包括了所有的文件中在'and'后面的单词。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  def mimic_dict(filename): \u0026#34;\u0026#34;\u0026#34;Returns mimic dict mapping each word to list of words which follow it.\u0026#34;\u0026#34;\u0026#34; # +++your code here+++ # LAB(begin solution) mimic_dict = {} f = open(filename, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) text = f.read() f.close() words = text.split() prev = \u0026#39;\u0026#39; for word in words: if not prev in mimic_dict: mimic_dict[prev] = [word] else: mimic_dict[prev].append(word) # Could write as: mimic_dict[prev] = mimic_dict.get(prev, []) + [word] # It\u0026#39;s one line, but not totally satisfying. prev = word return mimic_dict def print_mimic(mimic_dict, word): \u0026#34;\u0026#34;\u0026#34;Given mimic dict and start word, prints 200 random words.\u0026#34;\u0026#34;\u0026#34; # +++your code here+++ # LAB(begin solution) for unused_i in range(200): print(word,) nexts = mimic_dict.get(word) # Returns None if not found if not nexts: nexts = mimic_dict[\u0026#39;\u0026#39;] # Fallback to \u0026#39;\u0026#39; if not found word = random.choice(nexts) filename = \u0026#39;../data/sample_text.txt\u0026#39; mimic_dict = mimic_dict(filename) print_mimic(mimic_dict, \u0026#39;\u0026#39;)   "
},
{
	"uri": "https://lczen.github.io/en/categories/%E7%BB%83%E4%B9%A0%E9%A2%98/",
	"title": "练习题",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/pythonpractice3/",
	"title": "python练习题3",
	"tags": ["python练习题"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "文件读写练习题 请对“Shanghai.txt”文件进行读取，并统计其中每个单词(小写后)出现的词频，并以词频从高到低以如下形式写入\u0026quot;word_count.txt\u0026quot;中：\n单词1:频次 单词2:频次 ... 最后请输出10行结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import re,collections def get_words(in_file, out_file): with open (in_file, encoding=\u0026#39;utf-8\u0026#39;) as f: words_box=[] for line in f: words = line.strip().strip(\u0026#34;,\u0026#34;).strip(\u0026#34;.\u0026#34;).split() words = list(map(lambda x:x.lower(), words)) words_box.extend(words) word_dic = dict(collections.Counter(words_box)) word_count = sorted(word_dic.items(), key=lambda d: d[1], reverse=True) out = open(out_file, \u0026#39;w\u0026#39;) for word in word_count: out.write(word[0]+\u0026#34;:\u0026#34;+str(word[1])+\u0026#34;\\n\u0026#34;) out.close() get_words(\u0026#34;../data/ShangHai.txt\u0026#34;, \u0026#39;../../tmp/word_count.txt\u0026#39;)   正则表达式练习题 给出一段XX音乐中的HTML内容，用正则表达式取出html中所有的歌手名和歌名。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  # 取出html中的歌手名和歌名 html = \u0026#39;\u0026#39;\u0026#39;\u0026lt;div id=\u0026#34;songs-list\u0026#34;\u0026gt;\u0026lt;h2 class=\u0026#34;title\u0026#34;\u0026gt;经典老歌\u0026lt;/h2\u0026gt;\u0026lt;p class=\u0026#34;introduction\u0026#34;\u0026gt;经典老歌列表\u0026lt;/p\u0026gt;\u0026lt;ul id=\u0026#34;list\u0026#34;class=\u0026#34;list-group\u0026#34;\u0026gt;\u0026lt;li data-view=\u0026#34;2\u0026#34;\u0026gt;一路上有你\u0026lt;/li\u0026gt;\u0026lt;li data-view=\u0026#34;7\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/2.mp3\u0026#34;singer=\u0026#34;任贤齐\u0026#34;\u0026gt;沧海一声笑\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li data-view=\u0026#34;4\u0026#34;class=\u0026#34;active\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/3.mp3\u0026#34;singer=\u0026#34;齐秦\u0026#34;\u0026gt;往事随风\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li data-view=\u0026#34;6\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/4.mp3\u0026#34;singer=\u0026#34;beyond\u0026#34;\u0026gt;光辉岁月\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li data-view=\u0026#34;5\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/5.mp3\u0026#34;singer=\u0026#34;陈慧琳\u0026#34;\u0026gt;记事本\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li data-view=\u0026#34;5\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/6.mp3\u0026#34;singer=\u0026#34;邓丽君\u0026#34;\u0026gt;但愿人长久\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;\u0026#39;\u0026#39; import re def get_singer_songs(html_content): # 请完成题目要求 # your code here result = re.search(\u0026#39;\u0026lt;li.*?singer=\u0026#34;(.*?)\u0026#34;\u0026gt;(.*?)\u0026lt;/a\u0026gt;\u0026#39;, html_content, re.S) if result: print(result.group(1), result.group(2)) return result r = get_singer_songs(html) print(r)   "
},
{
	"uri": "https://lczen.github.io/en/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/",
	"title": "读书笔记",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/tags/%E9%9D%9E%E6%9A%B4%E5%8A%9B%E6%B2%9F%E9%80%9A/",
	"title": "非暴力沟通",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/readingnotes/",
	"title": "非暴力沟通",
	"tags": ["非暴力沟通"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "提出请求 请让我成为我自己。 5.如果你选择这一句，我们意见不一致。对我来说，“让我成为我自己”是一个模糊的请求。如果发言者说“我希望你告诉我，即使你不喜欢我做的一些事情，你仍然会和我在一起”，那么，我认为发言者提出了明确的请求。\n6.关于昨天的会议，请不要隐瞒你的看法。 6.如果你选择这一句，我们意见不一致。对我来说，“不要隐瞒”是一个模糊的请求。如果发言者说“请告诉我，你怎么看我昨天在会议中的表现，给我一些建议好吗”，那么，我认为发言者提出了明确的请求。\n马歇尔•卢森堡. 非暴力沟通 (Kindle 位置 953-955).\n7.我希望你能在规定的时速内驾驶。 8.我想更好地了解你。 8.如果你选择这一句，我们意见不一致。如果发言者说“我想多一些时间和你聊聊，不知道你是否愿意每周和我吃一次午饭”，那么，我认为发言者提出了明确的请求。\n9.我希望你尊重我的个人隐私。 9.如果你选择这一句，我们意见不一致。对我来说，“尊重我的个人隐私”这个短语并不能清楚地表达发言者的请求。如果发言者说“在进我的办公室前，请先敲门好吗”，那么，我认为发言者提出了明确的请求。\n10.我希望你经常做晚饭。 10.如果你选择这一句，我们意见不一致。对我来说，“经常”这个词并不能清楚地表达发言者的请求。如果发言者说“我希望你每周一晚上都可以做晚饭”，那么，我认为发言者提出了明确的请求。\n倾听 倾听之前的准备 为了倾听他人，我们需要先放下已有的想法和判断，全神贯注地体会对方。以色列哲学家马丁·布伯（MartinBuber）对此作出了描述：“尽管有种种相似之处，生活的每时每刻就像一个刚出生的婴儿，一张新的面孔，我们从未见过，也不可能再次见到。我们无法停留在过去，也无法预见我们的反应。我们需要不带成见地感受变化。我们需要用全身心去倾听。”\n倾听的作用\u0026mdash;工具人\n感受与需要 男：和你说话有什么用？你从不好好听。\n马歇尔：“你现在关心的是他的想法而非他的需要。我想，如果我们注意的是别人的需要而非他对我们的看法，我们将发现别人并不那么可怕。我们还会发现，他不高兴是因为他的需要没有得到满足。”\n（wrong）女：你感到不高兴，是因为你觉得我不理解你？\n女：你不高兴是因为你需要得到理解？\n给他人反馈 在倾听他人的观察、感受、需要和请求之后，我们可以主动表达我们的理解。如果我们已经准确领会了他们的意思，我们的反馈将帮助他们意识到这一点。反之，如果我们的理解还不到位，他们也就有机会来纠正我们。此外，这样做还有助于人们体会自己的状况，从而深入了解自己。\n使用疑问句给予反馈 非暴力沟通建议我们使用疑问句来给予他人反馈。这将便于他人对我们的理解作出必要的补充。问题可以集中于以下几个方面：\n1.他人的观察：“上周我有三个晚上不在家，你说的是这回事？”\n2.他人的感受及需要：“你很灰心？你希望得到肯定是吗？”\n3.他人的请求：“你是不是想请我帮你预订酒店？”\n以下没有用心体会他人：\n1.“你说的是什么事？”（我有些困惑。我想知道你是指哪件事。告诉我好吗？[感受+需要]）\n2.“你现在心情怎么样？”“为什么你会有那样的感觉？”\n3.“你希望我怎么做？”\n一般来说，如果一个人在说话时有明显的情绪，他一般会期待得到他人的反馈。\n只要我们专注于他人的感受和需要，所有的批评、攻击、辱骂或嘲讽就会消失。我们越是这样做，就越能体会到一个简单的事实：有时，我们认为自己受到了指责，实际上，那些话是他人表达需要和请求的方式。如果意识到这一点，我们就不会认为自己的人格受到了伤害。反之，如果一心分析自己或对方的过错，我们就会认为自己被贬低了。\n反馈是有价值的 wrong:\n太太：你从不好好听我讲话。\n丈夫：我哪里没有？\n太太：你就是没有！\nright:\n太太：你从不好好听我讲话。\n丈夫：听起来，你很失望。你需要体贴，是吗？\n太太：流下了眼泪\n保持关注 在谈话刚开始时，人们所表达的感受往往是冰山之一角，有许多相关的感受——通常是更为强烈的情感，并没有得到表达。倾听将为他们探究和表达内心深处的感受创造条件。反之，如果急于了解他们的请求或表达自己，就会妨碍这个过程。\n实例：母亲与我的对话\n母亲：不知道怎么回事，我的孩子不论我和他说什么，他都不听。\n我（表达理解和她的需要）：听起来，你很伤心，你希望找到和孩子沟通的办法。\n母亲：也许这是我的错。我总是冲他大喊大叫。”\n我（继续倾听并给予反馈）：你希望你能多体贴孩子，以前没有做到这一点，你现在有些内疚，是吗？\n母亲：我是一个失败的母亲。\n我（理解+反馈）：你有些灰心，你想加深与孩子的感情联系，是吗？\n我（不确定对方已经充分表达）：你还有什么话要告诉我吗？\n当我们痛苦得无法倾听 当我们痛苦得无法倾听他人时，我们需要\n（1）体会自己的感受和需要；\n（2）大声地提出请求；\n（3）换一个环境。\n倾听他人并给予反馈 1.甲：“我又误机了，我真是个混蛋！”乙：“没有人是十全十美的，不要太严格要求自己。” 你很失望，是因为你希望能够信赖自己，是吗？\n2.甲：“我认为我们应该把这些非法移民遣送回国。”乙：“这对改善社会治安有帮助吗？” 你有些担心，因为你很看重社会秩序和安全？\n3.甲：“你以为你什么都知道？！” 乙：“听起来，你有些不耐烦，因为你希望每个人的意见都能得到倾听？”\n4.甲：“你从不把我当回事。要不是我帮你，你自己一个人能处理这么多事情吗？”乙：“你怎么能这样想！我一直都很尊重你。” 你好像有些失落，你希望得到欣赏和肯定？\n5.甲：“你怎么可以那样和我说话？”乙：“我那样说话，你是不是很伤心？” 听起来，你很伤心，因为你需要体贴？\n6.甲：“想到我先生，我就有些气恼。我需要他的时候，他总是不在我身边。”乙：“你是希望他多陪陪你？” 听起来，你有些失落，因为你需要支持与关心？（反馈：感受+需要）\n7.甲：“我真受不了我自己，我现在变得这么胖！”乙：“慢跑也许会有帮助。” 你对自己好像有些不耐烦，你很看\n马歇尔•卢森堡. 非暴力沟通 (Kindle 位置 1178-1179).\n8.甲：“我紧张地筹备女儿的婚礼。可是，我亲家老是有新主意，真烦！”乙：“听起来，你有些着急，你希望能得到理解与配合，是吗？” 9.甲：“如果亲戚来之前不和我打招呼，我真的不想接待他们。”乙：“我知道这是什么感觉。我也这样。” 你是不是有些厌烦，你希望你的需要也能得到尊重，是吗？\n10.甲：“你的表现让我很失望。我本来指望你们部门上个月的产出能够翻番。”乙：“我知道你很失望。但上个月我们部门请病假的人很多。” 你看来很失望，你看重工作效率，是吗？\n第八章倾听的力量 倾听使身心痊愈 "
},
{
	"uri": "https://lczen.github.io/en/posts/websites/",
	"title": "资源下载",
	"tags": [""],
	"categories": [],
	"series": [""],
	"description": "",
	"content": "网站标签 http://www.yunpangou.com/2154977354010695\n"
},
{
	"uri": "https://lczen.github.io/en/posts/pythonpractice1/",
	"title": "pythn练习题1",
	"tags": ["python练习题"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "简答题 请谈谈Python中is 和 == 的区别 is用于比较两个变量是否引用了同一个内存地址 is 表示的是对象标示符（object identity），作用是用来检查对象的标示符是否一致，也就是比较两个对象在内存中的地址是否一样。\n==用于比较两个变量的值是否相等,== 表示的是相等（equality），是用来检查两个对象是否相等。\nis 是检查两个对象是否指向同一块内存空间，而 == 是检查他们的值是否相等。\n可以看出，is 是比 == 更严格的检查，is 返回True表明这两个对象指向同一块内存，值也一定相同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # case 1:对于数值型和字符串型对象, python的存储机制使得a和b其实指向同一个地址，所以is和==都是True a = 5 b = 5 print(\u0026#39;a is b:\u0026#39;, a is b) print(\u0026#39;a == b:\u0026#39;, a == b) print(\u0026#39;id(a):%s\\nid(b):%s\u0026#39;%(id(a),id(b))) print(\u0026#39;\\n\u0026#39;) # case 2:对于list、tuple等容器，a和b虽然内容一样，但是是两个不同的对象引用，指向两个不同的内存地址 a = [1,2,3] b = [1,2,3] print(\u0026#39;a is b:\u0026#39;, a is b) print(\u0026#39;a == b:\u0026#39;, a == b) print(\u0026#39;id(a):%s\\nid(b):%s\u0026#39;%(id(a),id(b))) print(\u0026#39;\\n\u0026#39;) # case 3：实际上可以通过修改对象的__eq__方法改变==的比较方式 class Student(): def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): # return self.name == other.name return False s1 = Student(\u0026#39;David\u0026#39;,18) s2 = Student(\u0026#39;David\u0026#39;,20) # 因为age不相同，很显然s1和s2是两个不同内容的对象，但是通过改写__eq__可以只通过比较名字来判断是否== print(\u0026#39;s1 is s2:\u0026#39;, a is b) print(\u0026#39;s1 == s2:\u0026#39;, a == b) print(\u0026#39;id(s1):%s\\nid(s2):%s\u0026#39;%(id(s1),id(s2)))   a is b: True\na == b: True\nid(a):1871024336\nid(b):1871024336\na is b: False\na == b: True\nid(a):3008335707656\nid(b):3008335707848\ns1 is s2: False\ns1 == s2: True\nid(s1):3008335765400\nid(s2):3008335765456\nPython中的高阶函数是指什么？ 提示：我们指的是map、reduce、filter这类函数\n高阶函数属于函数式编程中的一个概念。在python中函数是一个对象，这意味着可以使用其他变量引用函数。\npython里的高阶函数，即接受函数作为参数的函数。\npython中的常用的高阶函数有：\nmap: 可理解为对数据的做一对一映射\nreduce: 可理解为对数据做多对一的映射\nfilter: 对数据进行逐个过滤，仅保留符合条件的元素\nPython类中有哪些的成员，如何分类他们，他们各自又有哪些用途？ 提示：成员变量，成员函数\u0026hellip;\n数据成员\n类变量 : 在类中且函数体之外,实例之间共享\n实例变量 : 定义在init方法中,作用于当前实例的类\n方法成员\n类方法 : 用@classmethod装饰器,第一个参数为cls\n实例方法 : 绑定到实例的方法, 第一个参数为self\n静态方法 :\n 用@staticmethod装饰器\n没有cls self参数限制\n可以类名访问,也可以实例访问\n Python中列表，元组的相同之处及区别都有哪些？集合与字典呢？ 列表与元组\n相同点：\n列表与元组都是容器，是一系列的对象\n都可以包含任意类型的元素甚至可以是一个序列，还可以包含元素的顺序\n不同点：\n列表是可变的，而元组是不可变的，tuple 不可追加，tuple大小不可再改变。\n不能把列表当做字典的关键字，因为只有不可变的值才能进行哈希运算，因此只有不可变的值才能作为关键字。要使用列表做关键字，你需要把它转化为元组\n集合与字典\n相同点：\ndict和set当中都没有重复的key值，且key值必须是不可改变的对象。\n两者存储的元素都是无序的。\n不同点：\nset是一组key的集合，但不存储value；\ndict使用键-值（key-value）存储，具有极快的查找速度。\nPython中的模块和包是什么，如何自定义并使用？ 模块\n一个模块就是一个.py文件，里面定义了可以复用的函数和变量，当需要复用的时候就import导入进来进行复用\n自定义：创建一个.py文件就是文件名就是模块名称\n使用：使用import导入进来使用，搜索目录有当前路径、Bif中查找、安装路径、PYTHONPATH环境变量路径，找到该模块则可以导入并使用模块名.方法名的方式调用模块中的方法\n包\n就是多个模块或着多个包的集合（文件夹），避免由于开发者多而可能造成的模块命名重复，所以用包来统一管理模块\n自定义：给多个模块创建一个文件夹，文件夹内可以有模块（.py文件）或者文件夹（子包），并且需要包含一个init.py文件，用来做一些初始化工作和避免被解释器当作普通字符串\n使用：\n同样是使用import的方式导入到当前脚本中\n代码题 创建一个函数，接收一个字符串参数，判断其做为Python标识符是否合法。 具体要求：\n 如果合法则输出 True，否则输出 False。 如果该字符串与Python内置的关键字，或Bifs冲突，则打印'conflict\u0026rsquo; 注:Python标识符的规则，关键字和Bifs可搜索得到  1 2 3 4 5 6 7 8 9 10 11 12 13 14  import keyword,string def Identifier(s): kw = keyword.kwlist if s in kw or s in dir(__builtins__): print(\u0026#34;conflict\u0026#34;) elif s.isidentifier(): # isidentifier() 可用来判断变量名是否合法 print(\u0026#34;True\u0026#34;) else: print(\u0026#34;False\u0026#34;) Identifier(\u0026#34;list\u0026#34;) Identifier(\u0026#34;abc#\u0026#34;) Identifier(\u0026#34;1a\u0026#34;)   编写一个函数，能生成包含20个随机整数的列表，然后将前10个元素升序排列，后10个元素降序排列，并分别打印输出 提示：本题考察对列表的切片和排序操作，注意切片下标。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import random def sort2part(): ran_list = [] for i in range(20): ran_list.append(random.randint(0,100)) ran_list_pre_10 = ran_list[0:10] ran_list_pre_10.sort() ran_list_last_10 = ran_list[10::] ran_list_last_10.sort(reverse=True) print(\u0026#34;前十个随机数 升序: \u0026#34;+ str(ran_list_pre_10)) print(\u0026#34;后十个随机数 降序: \u0026#34; + str(ran_list_last_10)) sort2part()   有一分数序列为：2/1，3/2，5/3，8/5，13/8，21/13,请使用Python代码出生成并打印该数列的前30项并求和返回 提示：观察分数的分子和分母，每1项都和前2项有关系，也可检索“斐波那契数列”\n1 2 3 4 5 6 7 8 9 10 11  def fab(max): n, a, b = 0, 2, 1 result = 0 while n \u0026lt; max: print(\u0026#34;%d/%d\u0026#34;%(a,b)) result += a/b a, b = a + b, a n += 1 return result print(\u0026#34;该数列的前30项和: \u0026#34;,fab(30))    2/1\n3/2\n5/3\n8/5\n13/8\n21/13\n34/21\n55/34\n89/55\n144/89\n233/144\n377/233\n610/377\n987/610\n1597/987\n2584/1597\n4181/2584\n6765/4181\n10946/6765\n17711/10946\n28657/17711\n46368/28657\n75025/46368\n121393/75025\n196418/121393\n317811/196418\n514229/317811\n832040/514229\n1346269/832040\n2178309/1346269\n该数列的前30项和: 48.84060068717216\n BMI编写 身体质量指数（BMI）是根据人的体重和身高计算得出的一个数字，BMI是可靠的身体肥胖指标，其计算公式：BMI=Weight/High2，其中体重单位为公斤，身高单位为米。\n 计算公式为：$BMI=体重（kg）÷身高^2（m）$ 提示用户输入体重（kg）和身高的数字(m)(注意单位），然后计算BMI。 根据BMI指数范围，定义当前健康状态。BMI指数在18至25之间定义为健康的标准体重,小于该范围定义为偏瘦，超过该范围定义为偏重。 将BMI指数和其所代表状态输出  1 2 3 4 5 6 7 8 9 10 11  weight = float(input(\u0026#39;请输入您的体重：\u0026#39;)) high = float(input(\u0026#39;请输入您的身高：\u0026#39;)) BMI = weight / (high**2) weight_result = \u0026#34;健康的标准体重\u0026#34; if BMI \u0026gt; 25: weight_result = \u0026#34;偏重\u0026#34; elif BMI \u0026lt; 18: weight_result = \u0026#34;偏瘦\u0026#34; else: pass print(\u0026#39;您的BMI指数为%.2f，您的体重为%s\u0026#39;%(BMI,weight_result))   字符统计  创建一个函数，接收字符串输入，分别统计出其中英文字母、空格、数字和其它字符的个数后打印。 提示：  ord(\u0026lsquo;a\u0026rsquo;) 能将字符 \u0026lsquo;a\u0026rsquo; 转化为 ASCII 码表上对应的数值， 例如，空格为32 数字 0-9 对应的码值为 48-57 大写字母 A-Z 对应 65-90 小写字母 a-z 对应 97-122 比122高的都是其它。    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def str_count(s): count_a=count_z=count_o=count_s=0 for i in s: if (ord(i)\u0026gt;=97 and ord(i)\u0026lt;=122) or (ord(i)\u0026gt;=65 and ord(i)\u0026lt;=90): count_a=count_a+1 elif ord(i)\u0026gt;=48 and ord(i)\u0026lt;=57: count_z=count_z+1 elif ord(i)==32: count_s=count_s+1 else: count_o=count_o+1 print(\u0026#34;空格个数：%d个\u0026#34;%count_s) print(\u0026#34;数字个数：%d个\u0026#34;%count_z) print(\u0026#34;英文字母个数：%d个\u0026#34;%count_a) print(\u0026#34;其他字符个数：%d个\u0026#34;%count_o) str_count(\u0026#34;我是wei guo ying,今年27岁\u0026#34;)   创建一个函数，可以将去除给定列表中中相邻且重复的元素(只保留一个)后，打印输出结果。  说明  输入参数为 l1=[1,2,3,4,4,4,4,4,4,5,6,6,8,8,12,12,12,12,13] 操作后，保证原有整体排序不变，仅处理相邻且重复的元素 请勿使用set，否则该题不计分。    1 2 3 4 5 6 7 8 9 10 11 12 13  def remove_same(ll): cur = ll[0] remove_data = [] for i in range(1,len(ll)): if cur == ll[i]: remove_data.append(ll[i]) else: cur = ll[i] for j in range(len(remove_data)): ll.remove(remove_data[j]) return ll lst=[1,2,3,4,4,4,4,4,4,5,6,6,8,8,12,12,12,12,13] print(remove_same(lst))   创建一个函数，接收一个由整数组成的列表（需对输入列表做检查，长度最少为2,数据类型为整型），并检验后下列条件后输出：  如列表是升序排列的,则输出\u0026quot;ASC\u0026rdquo;; 如列表是降序排列的,则输出\u0026quot;DESC\u0026rdquo;; 如列表无序，则输出\u0026quot;WRONG\u0026rdquo;。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  def check_data(l): if len(l) \u0026lt; 2: print(\u0026#39;您输入的列表长度小于2，请重新输入\u0026#39;) else: check_list = list(filter(lambda x : True if isinstance(x,int) else False,l)) if len(l) == len(check_list): if l == sorted(l): print(\u0026#39;ASC\u0026#39;) elif l == sorted(l,reverse = True): print(\u0026#39;DESC\u0026#39;) else: print(\u0026#39;WRONG\u0026#39;) else: print(\u0026#34;列表中数据类型包含非整型，请重新输入\u0026#34;) check_data([1]) check_data([1,\u0026#34;2\u0026#34;]) check_data([1,2,3,4]) check_data([4,3,2,1]) check_data([1,3,2,4])   高阶函数综合运用 l1=[1,3,6,8,10,11,17]\n请仅使用map,reduce,filter对上方数组依次进行如下三次操作：\n 剔除掉所有的偶数后打印 对剩下的数字每个数字进行平方后打印 对数组求和后打印\n注意：每一题都是在上一问的基础上操作  1 2 3 4 5 6 7 8 9 10 11 12 13  #剔除掉所有的偶数后打印 l1 = [1,3,6,8,10,11,17] l2 = list(filter(lambda x:True if x%2 == 1 else False,l1)) print(l2) #对剩下的数字每个数字进行平方后打印 l3 = list(map(lambda x:pow(x,2),l2)) print(l3) #对数组求和后打印 from functools import reduce l4 = reduce(lambda x,y : x+y,l3) print(l4)   Python类设计 设计一个公司类，完成以下要求，并实例化不同对象进行验证\n类变量\n 类下公司的总个数，类下公司实例的名称列表\n类方法 返回公司类共有多少个公司实例 返回公司类的公司实例有名称列表\n实例变量 公司名，简介，利润，销售额，总成本，雇员姓名列表，雇员详细信息列表(这里可能会考察到*号对参数解包)\n实例方法： 招聘人才（每招一个人会有成本产生，影响该实例雇员列表、人数、总成本，默认成本cost=10000） 解雇人员（每解雇一个人会有成本产生，影响该实例雇员列表、人数 、总成本，默认成本cost=5000） 公司广告推广(影响该实例总成本，自定义成本cost) 交社保(按公司雇员总人数计算，影响该实例总成本，默认单人社保缴纳1000) 交税(按公司雇员总人数计算，影响该实例总成本，默认单人税费缴纳500) 销售（按销售件数*价格计算销售额，利润按销售额*利润率进行计算利润。默认利润率50%） 获取公司雇员列表 获取公司净利润  提示：具体的函数方法与变量定义，请参考下述实例化代码确定。初始化雇员的地方，大家看到调用的时候会包含年龄等信息，这里可能会用到不定长参数输入，详细可以参考python参数说明\n提示：写不出来的同学可以看一下上方的说明，实际上代码的结构跟上方说明结构基本一致\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  class Company(): companyNum = 0 companyList = [] def __init__(self, companyName, profile): self.companyName = companyName self.profile = profile self.profit = 0 self.sales = 0 self.totalCost = 0 self.employeeNames = [] self.employeeList = [] Company.companyNum += 1 Company.companyList.append(companyName) def recruit(self, name, *args, cost=10000): self.employeeNames.append(name) self.employeeList.append((name,*args)) self.totalCost += cost def dismiss(self, name, cost=5000): try: self.employeeNames.remove(name) for index in range(len(self.employeeList)): if self.employeeList[index][0]==name: self.employeeList.pop(index) self.totalCost += cost except ValueError: print(\u0026#39;没有name={}的员工，请重试\u0026#39;.format(name)) def adPromotion(self, cost): self.totalCost += cost def payInsurance(self, costPerPerson=1000): self.totalCost += costPerPerson * len(self.employeeNames) def payTax(self, costPerPerson=500): self.totalCost += costPerPerson * len(self.employeeNames) def sale(self, num, price, profit=0.5): self.sales += num * price self.profit += self.sales * profit def getEmployeeList(self): return self.employeeList def getProfit(self): return self.profit @classmethod def getCompanyNum(cls): return len(cls.companyList) @classmethod def getCompanyList(cls): return cls.companyList   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #功能验证 c0 = Company(\u0026#39;小象学院\u0026#39;,\u0026#39;大数据AI在线教育\u0026#39;) c0.recruit(\u0026#39;HanXiaoyang\u0026#39;,18, cost=20000) c0.recruit(\u0026#39;Eric\u0026#39;,20, cost=10000) print(\u0026#39;{}公司员工详细信息列表:{}\u0026#39;.format(c0.companyName,c0.getEmployeeList())) c0.dismiss(\u0026#39;Eric\u0026#39;) c0.adPromotion(5000) c0.payInsurance() c0.payTax() c0.sale(50,100) print(\u0026#39;{}公司员工详细信息列表:{}\u0026#39;.format(c0.companyName,c0.getEmployeeList())) print(\u0026#39;{}公司当前利润：{}\u0026#39;.format(c0.companyName,c0.getProfit())) c1 = Company(\u0026#39;百度\u0026#39;,\u0026#39;搜索引擎\u0026#39;) c1.recruit(\u0026#39;liyanhong\u0026#39;,30, cost=50000) c1.recruit(\u0026#39;likaifu\u0026#39;,50, cost=40000) print(\u0026#39;{}公司员工详细信息列表:{}\u0026#39;.format(c1.companyName,c1.getEmployeeList())) print(\u0026#39;公司名列表：\u0026#39;,Company.companyList) print(\u0026#39;公司总个数：\u0026#39;,Company.companyNum)   结合PIL库，制作一个能生成5位随机数验证码图片的函数。 生成5位随机数验证码并存储为verify.png名称的图片。\n提示：PIL库的使用方法可以参考python中PIL库的使用和PIL库简易教程与验证码生成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  %matplotlib inline from PIL import Image from PIL import ImageDraw from PIL import ImageFont import random def getRandomColor(): \u0026#39;\u0026#39;\u0026#39;获取一个随机颜色(r,g,b)格式的\u0026#39;\u0026#39;\u0026#39; c1 = random.randint(0,255) c2 = random.randint(0,255) c3 = random.randint(0,255) return (c1,c2,c3) # 获取一个Image对象，参数分别是RGB模式。宽150，高30，随机颜色 image = Image.new(\u0026#39;RGB\u0026#39;,(150,30),getRandomColor()) # 获取一个画笔对象，将图片对象传过去 draw = ImageDraw.Draw(image) # 获取一个font字体对象参数是ttf的字体文件的目录，以及字体的大小 font = ImageFont.truetype(\u0026#34;arial.ttf\u0026#34;, size=22) for i in range(5): # 循环5次，获取5个随机字符串 random_char = str(random.randint(0, 9)) print(random_char) # 在图片上一次写入得到的随机字符串,参数是：定位，字符串，颜色，字体 draw.text((10+i*30, 4),random_char,getRandomColor(),font) # 噪点噪线 width=180 height=30 # 划线 for i in range(5): x1=random.randint(0,width) x2=random.randint(0,width) y1=random.randint(0,height) y2=random.randint(0,height) draw.line((x1,y1,x2,y2),fill=getRandomColor()) # 画点 for i in range(30): draw.point([random.randint(0, width), random.randint(0, height)], fill=getRandomColor()) x = random.randint(0, width) y = random.randint(0, height) draw.arc((x, y, x + 5, y + 5), 0, 90, fill=getRandomColor()) # 保存到硬盘，名为verify.png格式为png的图片 image.save(open(\u0026#39;verify.png\u0026#39;, \u0026#39;wb\u0026#39;), \u0026#39;png\u0026#39;)    0\n4\n2\n1\n0\n 1 2 3 4 5 6 7 8  %matplotlib inline import matplotlib.pyplot as plt img = Image.open(\u0026#39;verify.png\u0026#39;) plt.figure(\u0026#39;\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.imshow(img) # %%html # \u0026lt;img src=\u0026#34;verify.png\u0026#34; width=\u0026#34;40%\u0026#34;\u0026gt;   "
},
{
	"uri": "https://lczen.github.io/en/showcase/hugo/",
	"title": "Hugo",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Hugo theme collection",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/showcase/",
	"title": "Showcase overview",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "My portfolio, repos, works overview page",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/categories/python%E5%9F%BA%E7%A1%80/",
	"title": "python基础",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/basicpractice/",
	"title": "python小练习",
	"tags": ["练习题"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "指定str位置，并更改该位置的值 1.实现一个功能，用户输入一个字符串，修改字符串中哪个位置，程序就会输出修改后的结果。（输入用input函数） 比如输入： 'fkjava.ipg' 6 - 程序输出：'fkjana-ipg' 我的理解：str不能根据index改变对应位置的值，而list可以。list转换成str用str的join函数。 1 2 3 4 5 6 7 8  s = input(\u0026#39;请输入一个字符串：\u0026#39;) k = input(\u0026#39;请输入要修改的位置和要修改成的符号，用空格分开：\u0026#39;) num = int(k[0]) # 要修改的index sub = k[2] # 要替换的字符 list_s = list(s) # 因为字符串是不可变的数据类型，我们字符串变成列表 list_s[num] = sub # 替换对应位置的字符 s = \u0026#39;\u0026#39;.join(list_s) print(\u0026#39;结果是：\u0026#39;, s)   找子串的数量 2.用户输入一个字符串和一个子串，程序会输出给定的子串在目标字符串中出现的次数。字符串遍历从左到右，而不是从右到左。 例如给定 'ABCDCDC' 和 'CDC' ，程序输出 '2' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #法一，利用str的find方法 s = input(\u0026#39;输入字符串\u0026#39;) sub_s = input(\u0026#39;输入子字符串\u0026#39;) index = 0 count = 0 for i in range(len(s)+1): if s.find(sub_s,index) != -1: count += 1 index = s.find(sub_s,s.find(sub_s,index)+1) print(count) #法二：关键点，s[i:i+len_ss] == ss s = input(\u0026#39;请输入一个字符串：\u0026#39;) ss = input(\u0026#39;请输入要查找的字串：\u0026#39;) num = 0 # 创建最后输出用的变量 len_ss = len(ss) # 我们的思路是通过切片来取出 s 中的一部分判断是否和子串 ss 一致，所以这里要先知道 ss 的长度 for i in range(len(s) - len_ss + 1): # 防止后面做切片的时候索引超出上界，这里我们先减掉 ss 的长度，最后一位数不被包括，所以 +1 if s[i] == ss[0]: # 判断每个字符和 ss 开头字符是否匹配 if s[i:i+len_ss] == ss: # 判断接下来的 len_ss 长度的字符是否与 ss 一致 num += 1 print(num)   把输入数据封装成元祖 3.提示用户输入N个字符串（N自己定义），将他们封装成元组，然后计算该元组乘以3再加上（'hello','world'）的结果。 keypoint:列表可以强转换成元组，元组+元组=元组 1 2 3 4 5  s = input(\u0026#39;请输入一些字符串，用空格隔开：\u0026#39;) list_s = s.split(\u0026#39;\u0026#39;) tuple_s = tuple(list_s) result = tuple_s * 3 + (\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;) print(result) # 主要观察结果的呈现形式，重点理解 元组 * 数字 的含义以及元组之间加法的用法   请输入一些字符串，用空格隔开：w ('w', 'w', 'w', 'hello', 'world') 列表append随机数 4.用户输入一个整数n，生成长度为n的列表，并将n个随机数放入列表中。（随机数可以参考random库，百度搜索 python random 即可） 1 2 3 4 5  num = int(input(\u0026#39;请用户输入一个整数：\u0026#39;))# 这里要注意input函数返回的是字符串类型，需要转成整型 l = [] # 定义一个空列表，用来装后面生成的随机数 for i in range(num): # 循环n次 l.append(r.random()) # 这里数字范围不限，可以用random库里不同的函数来实现 print(l)   将n个随机的奇数放入列表 5.用户输入一个整数n，生成长度为n的列表，并将n个随机的奇数放入列表中。 1 2 3 4 5 6 7 8 9  num = int(input(\u0026#39;请用户输入一个整数：\u0026#39;)) l = [0] * num # 这道题我们换一个思路，先生成列表，用0站位，然后我们替换里面的数字 for i in range(num): # 遍历l，对每个0做替换 while True: # while True 为死循环，当满足条件时我们用break打破循环 n = r.randint(1, 100) # 生成1-100之间的随机整数 if n%2 == 1: l[i] = n break print(l)   choice 6.用户输入一个整数n，生成长度为n的列表，并将n个随机的大写字符放入列表中。 1 2 3 4 5  num = int(input(\u0026#39;请用户输入一个整数：\u0026#39;)) l = [] for i in range(num): l.append(r.choice(\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;).upper()) # 用upper()变成大写，可以思考还有其他生成随机字符的方式吗？ print(l)   输出整数的最大值 7.用户输入以空格分隔的多个整数，输出这些整数的最大值。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  #法一 list.sort 列表表达式 nums = input(\u0026#39;请输入一些整数，以空格分隔：\u0026#39;) list_nums = nums.split(\u0026#39;\u0026#39;) list_int_nums = [int(item) for item in list_nums] list_int_nums.sort() print(list_int_nums[len(list_int_nums)-1]) #法二 nums = input(\u0026#39;请输入一些整数，以空格分隔：\u0026#39;) list_nums = nums.split(\u0026#39;\u0026#39;) list_int_nums = [] for i in range(len(list_nums)): # 第一个循环，我们把所有数字从字符串变成整型 num = int(list_nums[i]) list_int_nums.append(num) a = -99999999 for i in list_int_nums: # 第二个循环，a为目前最大数，遍历列表，如果当前数字比a大，把当前值赋值给a if i \u0026gt; a: a = i print(\u0026#39;最大的数为：\u0026#39;, a)   统计字符串中每个字母的次数* 8.用户输入N个大写字母，使用dict统计用户输入的每个字母的次数。 1 2 3 4 5 6 7 8  s = input(\u0026#39;请输入一些大写字母：\u0026#39;) d = dict() for i in s: if i in d.keys(): d[i] += 1 else: d[i] = 1 print(d)   输出等腰三角形 9.使用循环输出等腰三角形，层数由用户输入（input），输出结果如下（输入3）： * * *** keypoint:用数学方式去理解，*的规律是2n-1 1 2 3  n = int(input(\u0026#39;请输入三角形的层数：\u0026#39;)) for i in range(1, n+1): print((n-i)*\u0026#39;\u0026#39; + (2*i-1)*\u0026#39;*\u0026#39; + (n-i)*\u0026#39;\u0026#39;)   请输入三角形的层数：3 * *** ***** 九九乘法表 10.使用循环输出九九乘法表。输入结果如下： 1 * 1 = 1 1 * 2 = 2 2 * 2 = 4 ....... 1 2 3 4  for i in range(1, 10): for j in range(1, i+1): print(\u0026#39;{}x{}={}\\t\u0026#39;.format(j, i, i*j), end=\u0026#39;\u0026#39;) # \\t为制表符 print()   1x1=1\t1x2=2\t2x2=4\t1x3=3\t2x3=6\t3x3=9\t1x4=4\t2x4=8\t3x4=12\t4x4=16\t1x5=5\t2x5=10\t3x5=15\t4x5=20\t5x5=25\t1x6=6\t2x6=12\t3x6=18\t4x6=24\t5x6=30\t6x6=36\t1x7=7\t2x7=14\t3x7=21\t4x7=28\t5x7=35\t6x7=42\t7x7=49\t1x8=8\t2x8=16\t3x8=24\t4x8=32\t5x8=40\t6x8=48\t7x8=56\t8x8=64\t1x9=9\t2x9=18\t3x9=27\t4x9=36\t5x9=45\t6x9=54\t7x9=63\t8x9=72\t9x9=81\t"
},
{
	"uri": "https://lczen.github.io/en/tags/%E7%BB%83%E4%B9%A0%E9%A2%98/",
	"title": "练习题",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/regular/",
	"title": "Regular",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": "正则表达式 re模块 学习网站：http://regexr.com/\nPython通过re模块提供对正则表达式的支持。\n使用re的一般步骤是\n 1.将正则表达式的字符串形式编译为Pattern实例 2.使用Pattern实例处理文本并获得匹配结果（一个Match实例） 3.使用Match实例获得信息，进行其他的操作。  1 2 3 4 5 6 7 8 9 10 11 12  # encoding: UTF-8 import re # 将正则表达式编译成Pattern对象 pattern = re.compile(r\u0026#39;hello.*\\!\u0026#39;) # 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None match = pattern.match(\u0026#39;hello, hanxiaoyang! How are you?\u0026#39;) if match: # 使用Match获得分组信息 print(match.group())   re.compile(strPattern[, flag]):\n这个方法是Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象。\n第二个参数flag是匹配模式，取值可以使用按位或运算符'|'表示同时生效，比如re.I | re.M。\n当然，你也可以在regex字符串中指定模式，比如re.compile(\u0026lsquo;pattern\u0026rsquo;, re.I | re.M)等价于re.compile('(?im)pattern\u0026rsquo;)\nflag可选值有：\n re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同） re.M(MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图） re.S(DOTALL): 点任意匹配模式，改变\u0026rsquo;.\u0026lsquo;的行为 re.L(LOCALE): 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(UNICODE): 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。以下两个正则表达式是等价的：  1 2 3 4  regex_1 = re.compile(r\u0026#34;\u0026#34;\u0026#34;\\d + # 数字部分\\. # 小数点部分\\d * # 小数的数字部分\u0026#34;\u0026#34;\u0026#34;, re.X) regex_2 = re.compile(r\u0026#34;\\d+\\.\\d*\u0026#34;)   Match 小象学院《机器学习集训营》课程资料 by @寒小阳\nMatch对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息。\nmatch属性：\n string: 匹配时使用的文本。 re: 匹配时使用的Pattern对象。 pos: 文本中正则表达式开始搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。 endpos: 文本中正则表达式结束搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。 lastindex: 最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。 lastgroup: 最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。  match方法：\n group([group1, …]): 获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。 groups([default]): 以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。 groupdict([default]): 返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。 start([group]): 返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。 end([group]): 返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。 span([group]): 返回(start(group), end(group))。 expand(template): 将匹配到的分组代入template中然后返回。template中可以使用\\id或\\g、\\g引用分组，但不能使用编号0。\\id与\\g是等价的；但\\10将被认为是第10个分组，如果你想表达\\1之后是字符'0\u0026rsquo;，只能使用\\g\u0026lt;1\u0026gt;0。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import re m = re.match(r\u0026#39;(\\w+) (\\w+)(?P\u0026lt;sign\u0026gt;.*)\u0026#39;, \u0026#39;hello hanxiaoyang!\u0026#39;)#?P\u0026lt;sign\u0026gt;.*是标点的意思 print \u0026#34;m.string:\u0026#34;, m.string print \u0026#34;m.re:\u0026#34;, m.re print \u0026#34;m.pos:\u0026#34;, m.pos print \u0026#34;m.endpos:\u0026#34;, m.endpos print \u0026#34;m.lastindex:\u0026#34;, m.lastindex print \u0026#34;m.lastgroup:\u0026#34;, m.lastgroup print \u0026#34;m.group(1,2):\u0026#34;, m.group(1, 2) print \u0026#34;m.groups():\u0026#34;, m.groups() print \u0026#34;m.groupdict():\u0026#34;, m.groupdict() print \u0026#34;m.start(2):\u0026#34;, m.start(2) print \u0026#34;m.end(2):\u0026#34;, m.end(2) print \u0026#34;m.span(2):\u0026#34;, m.span(2) print r\u0026#34;m.expand(r\u0026#39;\\2 \\1\\3\u0026#39;):\u0026#34;, m.expand(r\u0026#39;\\2 \\1\\3\u0026#39;)   m.string: hello hanxiaoyang! m.re: \u0026lt;_sre.SRE_Pattern object at 0x10b111be0\u0026gt; m.pos: 0 m.endpos: 18 m.lastindex: 3 m.lastgroup: sign m.group(1,2): ('hello', 'hanxiaoyang') m.groups(): ('hello', 'hanxiaoyang', '!') m.groupdict(): {'sign': '!'} m.start(2): 6 m.end(2): 17 m.span(2): (6, 17) m.expand(r'\\2 \\1\\3'): hanxiaoyang hello! Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。 Pattern不能直接实例化，必须使用re.compile()进行构造。 Pattern提供了几个可读属性用于获取表达式的相关信息： * pattern: 编译时用的表达式字符串。 * flags: 编译时用的匹配模式。数字形式。 * groups: 表达式中分组的数量。 * groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。 1 2 3 4 5 6 7  import re p = re.compile(r\u0026#39;(\\w+) (\\w+)(?P\u0026lt;sign\u0026gt;.*)\u0026#39;, re.DOTALL) print \u0026#34;p.pattern:\u0026#34;, p.pattern print \u0026#34;p.flags:\u0026#34;, p.flags print \u0026#34;p.groups:\u0026#34;, p.groups print \u0026#34;p.groupindex:\u0026#34;, p.groupindex   p.pattern: (\\w+) (\\w+)(?P\u0026lt;sign\u0026gt;.*) p.flags: 16 p.groups: 3 p.groupindex: {'sign': 3} pattern匹配与替换  match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]): 这个方法将从string的pos下标处起尝试匹配pattern:  如果pattern结束时仍可匹配，则返回一个Match对象 如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。 pos和endpos的默认值分别为0和len(string)。 **注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符'$\u0026rsquo;。 **   search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]): 这个方法从string的pos下标处起尝试匹配pattern  如果pattern结束时仍可匹配，则返回一个Match对象 若无法匹配，则将pos加1后重新尝试匹配，直到pos=endpos时仍无法匹配则返回None。 pos和endpos的默认值分别为0和len(string))    1 2 3 4 5 6 7 8 9 10 11 12 13  # encoding: UTF-8  import re # 将正则表达式编译成Pattern对象  pattern = re.compile(r\u0026#39;H.*g\u0026#39;) # 使用search()查找匹配的子串，不存在能匹配的子串时将返回None  # 这个例子中使用match()无法成功匹配  match = pattern.search(\u0026#39;hello Hanxiaoyang!\u0026#39;) if match: # 使用Match获得分组信息  print match.group()#Hanxiaoyang    split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]):  按照能够匹配的子串将string分割后返回列表。 maxsplit用于指定最大分割次数，不指定将全部分割。    1 2 3 4  import re p = re.compile(r\u0026#39;\\d+\u0026#39;) print p.split(\u0026#39;one12306two2three3four4\u0026#39;)   ['one', 'two', 'three', 'four', '']  findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]):  搜索string，以列表形式返回全部能匹配的子串。    1 2 3 4  import re p = re.compile(r\u0026#39;\\d+\u0026#39;) print p.findall(\u0026#39;one12306two2three3four4\u0026#39;)   ['12306', '2', '3', '4']  **finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]): **  搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。    1 2 3 4 5  import re p = re.compile(r\u0026#39;\\d+\u0026#39;) for m in p.finditer(\u0026#39;one1two2three3four4\u0026#39;): print m.group()   1 2 3 4  **sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]): **  使用repl替换string中每一个匹配的子串后返回替换后的字符串。  当repl是一个字符串时，可以使用\\id或\\g、\\g引用分组，但不能使用编号0。 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。\ncount用于指定最多替换次数，不指定时全部替换。      1 2 3 4 5 6 7 8 9 10 11  import re p = re.compile(r\u0026#39;(\\w+) (\\w+)\u0026#39;) s = \u0026#39;i say, hello hanxiaoyang!\u0026#39; print p.sub(r\u0026#39;\\2 \\1\u0026#39;, s) def func(m): return m.group(1).title() + \u0026#39;\u0026#39; + m.group(2).title() print p.sub(func, s)   say i, hanxiaoyang hello! I Say, Hello Hanxiaoyang!  **subn(repl, string[, count]) |re.sub(pattern, repl, string[, count]): **  返回 (sub(repl, string[, count]), 替换次数)。    import re p = re.compile(r'(\\w+) (\\w+)') s = 'i say, hello hanxiaoyang!' print p.subn(r'\\2 \\1', s) def func(m): return m.group(1).title() + ' ' + m.group(2).title() print p.subn(func, s) ('say i, hanxiaoyang hello!', 2) ('I Say, Hello Hanxiaoyang!', 2) "
},
{
	"uri": "https://lczen.github.io/en/posts/python/",
	"title": "Python学习",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": "Python基础 Python运行方式 解释运行 新建py文件，直接运行\n1 2  %%writefile print_str.py print(\u0026#34;欢迎大家学习课程内容！\u0026#34;)   交互运行 ipython, notebook\nPython标识符 1 2 3 4  #如以下标识符取名错误 for=\u0026#39;chinahadoop.cn\u0026#39; 7a=8 str=\u0026#39;小象学院\u0026#39;   好的命名习惯   变量，函数或方法:salary,houseworker,booksname 常量：INTEREST_RATES 类：BankAccount,Employee,Company,OrderedDict #骆峰命名法   操作符优先级 第1名 - 函数调用、寻址、下标 第2名 - 幂运算 ** 第3名 - 翻转运算符 ~ 第4名 - 正负号 第5名 - *、/、% 第6名 - +、- 取商运算和开方 a//b #取商\na**0.5 #开方\n7.in和not in操作适用于str 1 2 3  websiteUrl=\u0026#39;chinahadoop.cn\u0026#39; if \u0026#39;.net\u0026#39; not in websiteUrl: print (\u0026#39;.net not in it\u0026#39;)   8.赋值操作 1 2 3  a=b=100 a,b,c=100,200,200 a,b=b,a#交换赋值   基本数据类型与数据结构 基本数据类型 类型：\nnumber：\nint,float,complex\nboolean,String,List,Tuple,Dictionary\n类型转换：\nint(a),str(a),eval(x),tuple(x),list(x),chr(x) # 整数转换为字符\n数值型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  #进制转换 a=0b0101001#bin() b=0o4#oct() c=0x28#hex() #浮点型 pi=3.1415 #科学计算法 c=5e13 print(c) #数字的正负无穷 #float(\u0026#39;inf\u0026#39;)正无穷 #float(\u0026#39;-inf\u0026#39;)负无穷 if 99999999999999999\u0026lt;float(\u0026#39;inf\u0026#39;): print(\u0026#39;you win!\u0026#39;) #复数 a=4.3+22j type(a) #数值类型转换 a=4.48 b=8 int(a) # 保留n位小数四舍五入 round(pi,3)   字符串型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  str2=\u0026#39;chinahadoop.cn\u0026#39; #str2[start🔚stride] #切片访问的特点：左闭右开[) str2[0::2] #从index为0到结尾，每隔2-1=1个字符取一次 #格式化访问1 companyName=\u0026#39;chinahadoop\u0026#39; str3=\u0026#39;http://{}.cn\u0026#39; print(str3.format(companyName)) #格式化访问2 print(\u0026#34;Company name is %s\u0026#34; %companyName) #格式化访问3 _=\u0026#39;Python 3.6\u0026#39;#fstring only can be used under version 3.6 str4=f\u0026#34;fstring is new feature of {_}\u0026#34; str4 #查找与替换 str3.find(\u0026#39;o\u0026#39;) str3.replace(\u0026#39;.cn\u0026#39;,\u0026#39;.net\u0026#39;) #统计 str3.count(\u0026#39;cn\u0026#39;)   数据结构 列表list 列表[]:任意元素类型对象的序列\n1 2 3 4 5 6 7 8 9  #append和extend # 列表追加 l2.append([\u0026#39;last\u0026#39;, \u0026#39;elem\u0026#39;]) #追加1个元素，这个元素是列表 l2.extend([\u0026#39;new\u0026#39;, \u0026#39;item\u0026#39;]) #追加列表 l2 + l1 #追加列表 # 列表删除元素 l2.pop() #返回最尾部元素，并删除该元素 l2.remove(\u0026#39;china\u0026#39;) #移除列表中的某个值的第一个匹配项，没有返回值   1 2 3 4 5  # 字符串列表的拼接与分割 l3 = [\u0026#39;I\u0026#39;, \u0026#39;love\u0026#39;, \u0026#39;China\u0026#39;] \u0026#34;_\u0026#34;.join(l3) #用下划线拼接列表元素(需要是字符串) a = \u0026#34;#\u0026#34;.join(l3) a.split(\u0026#34;#\u0026#34;) #用井号切分字符串生成列表   1 2 3 4 5 6 7 8 9 10 11 12 13  # 列表排序 my_list = [5,1,2,4,3] my_list.sort() #对my_list排序，直接改变my_list new_list = [5,1,2,4,3] sorted(new_list) #对new_list排序，以返回值返回排序结果，并不改变new_list # sorted高级用法 tmp_strs = [\u0026#39;aa\u0026#39;, \u0026#39;BBc\u0026#39;, \u0026#39;CCdd\u0026#39;, \u0026#39;zzmm\u0026#39;] sorted(tmp_strs) #按照字母序排序 sorted(tmp_strs, reverse=True) #按照字母序降序排序 sorted(tmp_strs, key=len) #根据key对元素做处理后的结果对原序列排序，这里的len是函数，返回字符串长度 sorted(tmp_strs, key=str.lower) #根据小写后的结果字母序排序 sorted(word_dic.items(), key=lambda d: d[1], reverse=True)#字典按照value排序   元组tuple 1 2 3 4 5 6 7 8 9  #元组()tuple,只读列表 t1=(1,2,3,4) #只读列表是真的只读吗？ t2=(1,2,3,[4,5,6]) t2[0]=11#报错 tup = (\u0026#39;physics\u0026#39;, \u0026#39;chemistry\u0026#39;, 1997, 2000) tup1 = (50,) #元组中只包含一个元素时，需要在元素后面添加逗号 tup[1:] #元组的切片拼接和list一样   集合set(去重非常好用) 1 2 3 4  #集合{}set：无序的不重复元素,集合中的对象，通常叫key s1={1,1,1,2,3,4,5,2,4,234} l2=[1,1,1,2,3,4,5,2,4,234] set(l2)   字典 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 字典是另一种可变容器模型，且可存储任意类型对象。 # 字典的每个键值 key=\u0026gt;value 对用冒号 : 分割，每个键值对之间用逗号 , 分割，整个字典包括在花括号 {} 中 my_dict = {\u0026#39;HanXiaoyang\u0026#39;: 1234, \u0026#39;Jason\u0026#39;: 4567, \u0026#39;David\u0026#39;: 6789} #定义字典 ks = my_dict.keys() #取出所有的key vs = my_dict.values() #取出所有的values Hv = my_dict[\u0026#39;HanXiaoyang\u0026#39;] #根据key取对应的value Dv = my_dict.get(\u0026#39;XiaoMing\u0026#39;, 2345) #根据key去取value，如果key不存在返回默认值 my_dict[\u0026#39;HanXiaoyang\u0026#39;] = 7890 #改变字典中key对应的value值 flag = \u0026#39;DaDa\u0026#39; in my_dict #判断是否有某个key # 遍历字典元素 for key in my_dict: print(key,my_dict[key]) from collections import OrderedDict # 注意，字典是一种键值对数据结构，本身是无序的，如果需要顺序，可以用OrderedDict od1=OrderedDict()#按主键首次插入顺序进行排序 od1[\u0026#39;a\u0026#39;]=7 od1[\u0026#39;z\u0026#39;]=8 od1[\u0026#39;b\u0026#39;]=9   声明Statement与Expression表达式 1 2 3 4 5 6 7 8 9 10 11  #statement声明 a=\u0026#39;小象学院\u0026#39;#statement通常为赋值语句 b=100 c=a #Expression表达式 #值、变量和运算符共同组成的整体我们将其称为表达式。通常有值表达式、变量表达式、计算表达式、字符串表达式, b,b+100,a+\u0026#39;is amazing\u0026#39;,__builtins__ #exec执行声明语句,eval执行表达式 exec(\u0026#39;a=5\u0026#39;) b=3 eval(\u0026#39;a+b+5\u0026#39;)   判断：if， if else ,if elif elif与三元表达式 1  \u0026#39;a\u0026#39; if age\u0026gt;50 else \u0026#39;b\u0026#39;   列表推导式List Comprehensions，可迭代对象Iterable与迭代器iterator,生成器generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #列表推导式：从一个列表中生成一个新的列表，简化了循环的写法 l1= [x+1 for x in range(30) if x%3==0] #新的list中的元素，来源于从0-29这个list中所有整除3的元素+1 l1=[0, 3, 6, 9, 12, 15, 18, 21, 24, 27] l2=iter(l1) l2.__next__() #可迭代对象Iterable：可以被for循环的对象统称为可迭代对象Iterable,list,str,dict这些都是可迭代类型 #迭代器Iterator：可以被next调用的迭代器。 #next(l1) #TypeError: \u0026#39;list\u0026#39; object is not an iterator #使用iter将一来个可迭代对象变为迭代器 l1=iter(l1) next(l1),next(l1) #生成器Generator：首先是一个迭代器，然后其内容是按需生成 #列表是一次性生成，缺点是当内容过大时会占用大量内容，那能不能用到多少生成多少呢？ #Python里一边循环一边计算(惰性计算)的迭代器称为生成器（Generator） #1.直接从列表表达式生成 g1= (x**2 for x in range(30) if x%2==0) type(g1) #\u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; next(g1),next(g1),next(g1),next(g1),g1.__next__(),g1.__next__() #2.函数生成，与yield关键字 def g2_func(n): for i in range(n): yield i**2 g2=g2_func(7) next(g2),next(g2),g2.__next__(),g2.__next__(),g2.__next__(),g2.__next__() #yield from/子迭代器，后面直接是可迭代对象。 def yield_from_iter(iter_object): yield from iter_object y1=yield_from_iter(\u0026#39;China\u0026#39;) y1.__next__(),next(y1)   处理异常 1 2 3 4 5 6 7 8 9 10 11 12  #try-finally 无法是否捕获了异常，都会执行finally后的语句 x = 5 #y = 2 y = 0 try: result = x / y except ZeroDivisionError: print(\u0026#34;division by zero!\u0026#34;) else: print(\u0026#34;result is\u0026#34;, result) finally: print(\u0026#34;executing finally clause\u0026#34;)   函数定义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # 定义斐波那契数列 def fib(n): a, b =1, 1 for i in range(n): print(a, end=\u0026#34;\u0026#34;) a,b = b,a+b print() # 函数默认参数 def printinfo(name, age = 35): print(\u0026#34;姓名:\u0026#34;, name) print(\u0026#34;年龄:\u0026#34;, age) return #调用printinfo函数 printinfo(age=50, name=\u0026#34;隔壁老王\u0026#34;); printinfo(name=\u0026#34;隔壁老王\u0026#34;); def printinfo(arg1, *vartuple): \u0026#34;打印任何传入的参数\u0026#34; print(\u0026#34;输出:\u0026#34;) print(arg1) print(vartuple) return # 调用printinfo 函数 printinfo(10) printinfo(70, 60, 50) # python 使用 lambda 来创建匿名函数。lambda [arg1,arg2,.....argn]:expression sum = lambda arg1, arg2: arg1 + arg2; # 调用sum函数 print(\u0026#34;相加后的值为:\u0026#34;, sum( 10, 20 )) print(\u0026#34;相加后的值为:\u0026#34;, sum( 20, 30 ))   filter和map和reduce 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #filter()函数接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。 #判断是否为正数 def is_positive(x): return x \u0026gt; 0 result = filter(is_positive,[1,3,5,-1,-10,0]) list(result) #map()是 Python 内置的高阶函数，它接收一个函数 f 和一个 list，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回。 ## 每个元素求平方 def f(x): return x * x result = map(f,[1,3,5,-1,-10,0]) list(result) #reduce()函数也是Python内置的一个高阶函数。reduce()函数接收的参数和 map()类似，一个函数 f，一个list，但行为和 map()不同，reduce()传入的函数 f 必须接收两个参数，reduce()对list的每个元素反复调用函数f，并返回最终结果值。 from functools import reduce ## 求和 def add(x,y): return x + y result = reduce(add,[1,3,5,-1,-10,0])   counter函数 1  word_dic = dict(collections.Counter(words_box))#统计列表words_box中词频并转成字典   文件操作 文件读 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # 要以读文件的模式打开一个文件对象，使用Python内置的`open()`函数，传入文件名和标示符： f = open(\u0026#39;print_str.py\u0026#39;, \u0026#39;r\u0026#39;) # 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： f.read() # 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源 f.close() # 异常处理模式 try: f = open(\u0026#39;/path/to/file\u0026#39;, \u0026#39;r\u0026#39;) print(f.read()) except Exception as e: print e finally: if f: f.close() # 逐行读取 with open(\u0026#39;test.py\u0026#39;, \u0026#39;r\u0026#39;) as f: for line in f.readlines(): print(line.strip())   文件写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  with open(\u0026#39;test.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;Hello, world!\\n\u0026#39;) f.write(\u0026#39;new line\u0026#39;) lines = [\u0026#39;Hello, python!\\n\u0026#39;, \u0026#39;New text!\u0026#39;] with open(\u0026#39;new.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.writelines(lines) # 需要用到os内置模块 import os # 列举当前绝对路径 os.path.abspath(\u0026#39;.\u0026#39;) # 新建文件夹 os.mkdir(\u0026#39;./testdir\u0026#39;) # 文件重命名 os.rename(\u0026#39;test.txt\u0026#39;, \u0026#39;test2.txt\u0026#39;) # 列出当前路径下的所有文件夹 [x for x in os.listdir(\u0026#39;.\u0026#39;) if os.path.isdir(x)] # 列出当前路径下的所有python脚本文件 [x for x in os.listdir(\u0026#39;.\u0026#39;) if os.path.isfile(x) and os.path.splitext(x)[1]==\u0026#39;.py\u0026#39;]   面向对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  # 类的设计与创建 # 使用 class 语句来创建一个新类，class 之后为类的名称并以冒号结尾，类的主体由类成员，方法，数据属性组成 class Employee: \u0026#34;所有员工的基类\u0026#34; empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print(\u0026#34;总共雇员数量 %d\u0026#34; % Employee.empCount) def displayEmployee(self): print(\u0026#34;姓名: \u0026#34;, self.name, \u0026#34;, 工资: \u0026#34;, self.salary) # empCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。 # 第一种方法__init__()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法 # self 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。 # 创建对象 \u0026#34;创建 Employee 类的第一个对象\u0026#34; emp1 = Employee(\u0026#34;张三\u0026#34;, 2000) \u0026#34;创建 Employee 类的第二个对象\u0026#34; emp2 = Employee(\u0026#34;李四\u0026#34;, 5000) # 调用成员函数 emp1.displayEmployee() emp2.displayEmployee() print(\u0026#34;总共雇员数%d\u0026#34; % Employee.empCount)   "
},
{
	"uri": "https://lczen.github.io/en/posts/jupyter%E6%93%8D%E4%BD%9C%E6%8A%80%E5%B7%A7/",
	"title": "jupyter小技巧",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": "1.jupyter安装 1 2 3 4  conda install jupyter notebook # 安装anaconda之后，下载jupyter notebook conda install -c conda-forge jupyter_contrib_nbextensions # jupyter notebook扩展安装 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple autopep8 # 代码自动规范化包   2.jupyter magic关键字 %quickref %lsmagic #查看magic有哪些magic命令 #line magic %config ZMQInteractiveShell.ast_node_interactivity='all'#这里是打印多行的开关 off打印多行 %pprint #cell magic %%writefile test.py #把代码写进py文件 for item in range(100): print(item) %%timeit 100 #100,000执行的时间 !python -m pip install jupyter_contrib_nbextensions --user !jupyter contrib nbextension install --user --skip-running-check #这里报错，使用conda安装即可 !conda install -c conda-forge jupyter_contrib_nbextensions !pip install --user jupyter_nbextensions_configurator !jupyter nbextensions_configurator enable --user #jupyter 扩展下载，支持目录 "
},
{
	"uri": "https://lczen.github.io/en/categories/hugo%E5%AE%89%E8%A3%85/",
	"title": "hugo安装",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lczen.github.io/en/posts/hugo%E5%AE%89%E8%A3%85%E7%BD%91%E7%AB%99%E6%8C%87%E4%BB%A4/",
	"title": "hugo安装指令",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": "hugo安装指令 root目录下config.toml中的头部要改成： baseURL = \u0026quot;/\u0026quot; themesDir = \u0026quot;themes\u0026quot; languageCode = \u0026quot;en-us\u0026quot; title = \u0026quot;Zen LU\u0026quot; theme = \u0026quot;m10c\u0026quot; brew install hugo hugo new site zenblog cd zenblog git clone https://github.com/vaga/hugo-theme-m10c.git themes/m10c cd themes ls cd .. hugo server -t m10c --buildDrafts hugo new post/blog.md hugo --theme=m10c --baseUrl=\u0026quot;https://lczen.github.io/\u0026quot; --buildDrafts cd public git init git add . git commit -m \u0026quot;我的hugo文件第一次提交\u0026quot; git remote add origin https://github.com/lczen/lczen.github.io.git git pull origin master --allow-unrelated-histories(我第二次换主题的时候用了这个) git push -u origin master #后面每次更新直接用下面代码： 修改以后： cd /users/zen/xiaolou hugo --theme=zzo --baseUrl=\u0026quot;https://lczen.github.io/\u0026quot; --buildDrafts cd public git init git add . git remote add origin https://github.com/lczen/lczen.github.io.git git commit -m “justmac” git pull origin master --allow-unrelated-histories git push -u origin master markdown显示图片 "
},
{
	"uri": "https://lczen.github.io/en/archive/",
	"title": "Archive",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Archive Page",
	"content": "archive page\n"
},
{
	"uri": "https://lczen.github.io/en/about/",
	"title": "About",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Hugo, the world’s fastest framework for building websites",
	"content": "Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n"
},
{
	"uri": "https://lczen.github.io/en/pt/",
	"title": "Presentations",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Presentation list with reveal.js",
	"content": ""
}]